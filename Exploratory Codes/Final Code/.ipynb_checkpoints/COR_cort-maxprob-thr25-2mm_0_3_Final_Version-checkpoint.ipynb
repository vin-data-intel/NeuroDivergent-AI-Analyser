{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3ae8f9-b291-4803-ba79-6e5ac46e08ff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Threshold----\n",
      "cort-maxprob-thr25-2mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/889 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/889 [00:06<1:33:33,  6.32s/it]\u001b[A\n",
      "  0%|          | 2/889 [00:08<59:12,  4.01s/it]  \u001b[A\n",
      "  0%|          | 3/889 [00:14<1:09:09,  4.68s/it]\u001b[A\n",
      "  0%|          | 4/889 [00:17<1:02:35,  4.24s/it]\u001b[A\n",
      "  1%|          | 5/889 [00:20<56:51,  3.86s/it]  \u001b[A\n",
      "  1%|          | 6/889 [00:27<1:10:03,  4.76s/it]\u001b[A\n",
      "  1%|          | 8/889 [00:34<1:00:03,  4.09s/it]\u001b[A\n",
      "  1%|          | 9/889 [00:38<58:52,  4.01s/it]  \u001b[A\n",
      "  1%|          | 10/889 [00:42<1:00:33,  4.13s/it]\u001b[A\n",
      "  1%|          | 11/889 [00:45<57:09,  3.91s/it]  \u001b[A\n",
      "  1%|▏         | 12/889 [00:52<1:07:30,  4.62s/it]\u001b[A\n",
      "  1%|▏         | 13/889 [00:58<1:12:21,  4.96s/it]\u001b[A\n",
      "  2%|▏         | 14/889 [01:01<1:07:20,  4.62s/it]\u001b[A\n",
      "  2%|▏         | 15/889 [01:06<1:08:19,  4.69s/it]\u001b[A\n",
      "  2%|▏         | 16/889 [01:13<1:18:50,  5.42s/it]\u001b[A\n",
      "  2%|▏         | 17/889 [01:16<1:05:19,  4.49s/it]\u001b[A\n",
      "  2%|▏         | 18/889 [01:24<1:20:36,  5.55s/it]\u001b[A\n",
      "  2%|▏         | 20/889 [01:30<1:03:03,  4.35s/it]\u001b[A\n",
      "  2%|▏         | 22/889 [01:34<49:42,  3.44s/it]  \u001b[A\n",
      "  3%|▎         | 24/889 [01:37<41:29,  2.88s/it]\u001b[A\n",
      "  3%|▎         | 25/889 [01:40<40:35,  2.82s/it]\u001b[A\n",
      "  3%|▎         | 26/889 [01:45<47:06,  3.27s/it]\u001b[A\n",
      "  3%|▎         | 27/889 [01:52<1:01:10,  4.26s/it]\u001b[A\n",
      "  3%|▎         | 28/889 [01:55<54:29,  3.80s/it]  \u001b[A\n",
      "  3%|▎         | 30/889 [02:00<47:19,  3.31s/it]\u001b[A\n",
      "  4%|▎         | 33/889 [02:04<33:05,  2.32s/it]\u001b[A\n",
      "  4%|▍         | 34/889 [02:11<47:18,  3.32s/it]\u001b[A\n",
      "  4%|▍         | 36/889 [02:14<37:09,  2.61s/it]\u001b[A\n",
      "  4%|▍         | 37/889 [02:20<46:08,  3.25s/it]\u001b[A\n",
      "  4%|▍         | 38/889 [02:22<41:46,  2.95s/it]\u001b[A\n",
      "  4%|▍         | 39/889 [02:26<46:38,  3.29s/it]\u001b[A\n",
      "  4%|▍         | 40/889 [02:30<48:58,  3.46s/it]\u001b[A\n",
      "  5%|▍         | 41/889 [02:37<1:00:42,  4.30s/it]\u001b[A\n",
      "  5%|▍         | 42/889 [02:38<50:22,  3.57s/it]  \u001b[A\n",
      "  5%|▍         | 44/889 [02:41<37:26,  2.66s/it]\u001b[A\n",
      "  5%|▌         | 45/889 [02:47<46:16,  3.29s/it]\u001b[A\n",
      "  5%|▌         | 47/889 [02:49<35:18,  2.52s/it]\u001b[A\n",
      "  5%|▌         | 48/889 [02:55<44:03,  3.14s/it]\u001b[A\n",
      "  6%|▌         | 49/889 [03:00<52:31,  3.75s/it]\u001b[A\n",
      "  6%|▌         | 50/889 [03:06<1:00:51,  4.35s/it]\u001b[A\n",
      "  6%|▌         | 51/889 [03:12<1:04:51,  4.64s/it]\u001b[A\n",
      "  6%|▌         | 52/889 [03:18<1:11:36,  5.13s/it]\u001b[A\n",
      "  6%|▌         | 53/889 [03:21<1:03:30,  4.56s/it]\u001b[A\n",
      "  6%|▌         | 54/889 [03:25<59:50,  4.30s/it]  \u001b[A\n",
      "  6%|▌         | 55/889 [03:30<1:04:21,  4.63s/it]\u001b[A\n",
      "  6%|▋         | 56/889 [03:34<59:10,  4.26s/it]  \u001b[A\n",
      "  6%|▋         | 57/889 [03:39<1:02:41,  4.52s/it]\u001b[A\n",
      "  7%|▋         | 60/889 [03:45<42:16,  3.06s/it]  \u001b[A\n",
      "  7%|▋         | 61/889 [03:50<49:01,  3.55s/it]\u001b[A\n",
      "  7%|▋         | 62/889 [03:54<51:00,  3.70s/it]\u001b[A\n",
      "  7%|▋         | 64/889 [03:58<40:06,  2.92s/it]\u001b[A\n",
      "  7%|▋         | 65/889 [04:03<48:02,  3.50s/it]\u001b[A\n",
      "  7%|▋         | 66/889 [04:08<50:45,  3.70s/it]\u001b[A\n",
      "  8%|▊         | 67/889 [04:15<1:03:30,  4.64s/it]\u001b[A\n",
      "  8%|▊         | 68/889 [04:22<1:12:33,  5.30s/it]\u001b[A\n",
      "  8%|▊         | 69/889 [04:27<1:12:02,  5.27s/it]\u001b[A\n",
      "  8%|▊         | 70/889 [04:32<1:10:08,  5.14s/it]\u001b[A\n",
      "  8%|▊         | 71/889 [04:35<1:02:23,  4.58s/it]\u001b[A\n",
      "  8%|▊         | 72/889 [04:41<1:06:25,  4.88s/it]\u001b[A\n",
      "  8%|▊         | 73/889 [04:44<58:03,  4.27s/it]  \u001b[A\n",
      "  8%|▊         | 74/889 [04:45<47:37,  3.51s/it]\u001b[A\n",
      "  8%|▊         | 75/889 [04:51<55:19,  4.08s/it]\u001b[A\n",
      "  9%|▊         | 76/889 [04:55<54:38,  4.03s/it]\u001b[A\n",
      "  9%|▊         | 77/889 [04:58<52:47,  3.90s/it]\u001b[A\n",
      "  9%|▉         | 78/889 [05:04<1:01:26,  4.55s/it]\u001b[A\n",
      "  9%|▉         | 79/889 [05:10<1:03:58,  4.74s/it]\u001b[A\n",
      "  9%|▉         | 80/889 [05:15<1:06:37,  4.94s/it]\u001b[A\n",
      "  9%|▉         | 81/889 [05:22<1:16:03,  5.65s/it]\u001b[A\n",
      "  9%|▉         | 82/889 [05:26<1:08:24,  5.09s/it]\u001b[A\n",
      "  9%|▉         | 83/889 [05:32<1:12:35,  5.40s/it]\u001b[A\n",
      "  9%|▉         | 84/889 [05:38<1:16:04,  5.67s/it]\u001b[A\n",
      " 10%|▉         | 85/889 [05:42<1:05:31,  4.89s/it]\u001b[A\n",
      " 10%|▉         | 86/889 [05:46<1:02:47,  4.69s/it]\u001b[A\n",
      " 10%|▉         | 87/889 [05:51<1:03:07,  4.72s/it]\u001b[A\n",
      " 10%|█         | 89/889 [05:53<41:49,  3.14s/it]  \u001b[A\n",
      " 10%|█         | 90/889 [05:58<47:59,  3.60s/it]\u001b[A\n",
      " 10%|█         | 91/889 [06:02<48:22,  3.64s/it]\u001b[A\n",
      " 10%|█         | 92/889 [06:04<44:37,  3.36s/it]\u001b[A\n",
      " 10%|█         | 93/889 [06:11<54:28,  4.11s/it]\u001b[A\n",
      " 11%|█         | 94/889 [06:14<53:04,  4.01s/it]\u001b[A\n",
      " 11%|█         | 95/889 [06:22<1:06:17,  5.01s/it]\u001b[A\n",
      " 11%|█         | 96/889 [06:29<1:15:38,  5.72s/it]\u001b[A\n",
      " 11%|█         | 98/889 [06:34<54:20,  4.12s/it]  \u001b[A\n",
      " 11%|█▏        | 101/889 [06:38<37:06,  2.82s/it]\u001b[A\n",
      " 12%|█▏        | 103/889 [06:41<31:33,  2.41s/it]\u001b[A\n",
      " 12%|█▏        | 104/889 [06:45<35:43,  2.73s/it]\u001b[A\n",
      " 12%|█▏        | 105/889 [06:52<47:26,  3.63s/it]\u001b[A\n",
      " 12%|█▏        | 107/889 [06:59<46:16,  3.55s/it]\u001b[A\n",
      " 12%|█▏        | 109/889 [07:03<38:56,  3.00s/it]\u001b[A\n",
      " 12%|█▏        | 110/889 [07:12<53:12,  4.10s/it]\u001b[A\n",
      " 12%|█▏        | 111/889 [07:16<53:03,  4.09s/it]\u001b[A\n",
      " 13%|█▎        | 112/889 [07:23<1:04:03,  4.95s/it]\u001b[A\n",
      " 13%|█▎        | 113/889 [07:30<1:08:42,  5.31s/it]\u001b[A\n",
      " 13%|█▎        | 114/889 [07:37<1:15:38,  5.86s/it]\u001b[A\n",
      " 13%|█▎        | 115/889 [07:46<1:26:21,  6.69s/it]\u001b[A\n",
      " 13%|█▎        | 116/889 [07:51<1:18:38,  6.10s/it]\u001b[A\n",
      " 13%|█▎        | 117/889 [07:55<1:11:57,  5.59s/it]\u001b[A\n",
      " 13%|█▎        | 118/889 [08:01<1:15:12,  5.85s/it]\u001b[A\n",
      " 13%|█▎        | 120/889 [08:04<49:39,  3.87s/it]  \u001b[A\n",
      " 14%|█▍        | 123/889 [08:09<34:56,  2.74s/it]\u001b[A\n",
      " 14%|█▍        | 125/889 [08:13<32:49,  2.58s/it]\u001b[A\n",
      " 14%|█▍        | 126/889 [08:16<31:40,  2.49s/it]\u001b[A\n",
      " 14%|█▍        | 127/889 [08:23<44:18,  3.49s/it]\u001b[A\n",
      " 14%|█▍        | 128/889 [08:27<46:19,  3.65s/it]\u001b[A\n",
      " 15%|█▍        | 129/889 [08:31<47:29,  3.75s/it]\u001b[A\n",
      " 15%|█▍        | 130/889 [08:38<56:26,  4.46s/it]\u001b[A\n",
      " 15%|█▍        | 132/889 [08:42<43:46,  3.47s/it]\u001b[A\n",
      " 15%|█▍        | 133/889 [08:46<46:06,  3.66s/it]\u001b[A\n",
      " 15%|█▌        | 134/889 [08:50<47:30,  3.78s/it]\u001b[A\n",
      " 15%|█▌        | 136/889 [08:55<40:55,  3.26s/it]\u001b[A\n",
      " 16%|█▌        | 138/889 [09:00<36:25,  2.91s/it]\u001b[A\n",
      " 16%|█▌        | 140/889 [09:09<42:30,  3.40s/it]\u001b[A\n",
      " 16%|█▌        | 141/889 [09:13<44:09,  3.54s/it]\u001b[A\n",
      " 16%|█▌        | 142/889 [09:16<43:06,  3.46s/it]\u001b[A\n",
      " 16%|█▌        | 143/889 [09:23<55:06,  4.43s/it]\u001b[A\n",
      " 16%|█▌        | 144/889 [09:29<59:14,  4.77s/it]\u001b[A\n",
      " 16%|█▋        | 145/889 [09:32<51:33,  4.16s/it]\u001b[A\n",
      " 16%|█▋        | 146/889 [09:38<59:46,  4.83s/it]\u001b[A\n",
      " 17%|█▋        | 148/889 [09:43<47:36,  3.85s/it]\u001b[A\n",
      " 17%|█▋        | 149/889 [09:51<59:22,  4.81s/it]\u001b[A\n",
      " 17%|█▋        | 150/889 [09:54<51:42,  4.20s/it]\u001b[A\n",
      " 17%|█▋        | 152/889 [09:56<34:36,  2.82s/it]\u001b[A\n",
      " 17%|█▋        | 153/889 [09:58<33:36,  2.74s/it]\u001b[A\n",
      " 17%|█▋        | 155/889 [10:06<39:22,  3.22s/it]\u001b[A\n",
      " 18%|█▊        | 156/889 [10:12<46:55,  3.84s/it]\u001b[A\n",
      " 18%|█▊        | 157/889 [10:18<53:13,  4.36s/it]\u001b[A\n",
      " 18%|█▊        | 158/889 [10:23<54:53,  4.51s/it]\u001b[A\n",
      " 18%|█▊        | 159/889 [10:30<1:04:25,  5.30s/it]\u001b[A\n",
      " 18%|█▊        | 160/889 [10:34<58:23,  4.81s/it]  \u001b[A\n",
      " 18%|█▊        | 161/889 [10:41<1:06:13,  5.46s/it]\u001b[A\n",
      " 18%|█▊        | 163/889 [10:50<1:01:03,  5.05s/it]\u001b[A\n",
      " 18%|█▊        | 164/889 [10:55<1:00:30,  5.01s/it]\u001b[A\n",
      " 19%|█▊        | 165/889 [10:59<58:24,  4.84s/it]  \u001b[A\n",
      " 19%|█▊        | 166/889 [11:02<51:07,  4.24s/it]\u001b[A\n",
      " 19%|█▉        | 167/889 [11:08<57:42,  4.80s/it]\u001b[A\n",
      " 19%|█▉        | 168/889 [11:12<53:23,  4.44s/it]\u001b[A\n",
      " 19%|█▉        | 169/889 [11:14<46:05,  3.84s/it]\u001b[A\n",
      " 19%|█▉        | 170/889 [11:18<46:03,  3.84s/it]\u001b[A\n",
      " 19%|█▉        | 171/889 [11:24<54:11,  4.53s/it]\u001b[A\n",
      " 19%|█▉        | 172/889 [11:33<1:10:50,  5.93s/it]\u001b[A\n",
      " 19%|█▉        | 173/889 [11:38<1:05:10,  5.46s/it]\u001b[A\n",
      " 20%|█▉        | 174/889 [11:47<1:19:13,  6.65s/it]\u001b[A\n",
      " 20%|█▉        | 176/889 [11:52<55:17,  4.65s/it]  \u001b[A\n",
      " 20%|█▉        | 177/889 [11:57<56:17,  4.74s/it]\u001b[A\n",
      " 20%|██        | 178/889 [12:03<1:00:35,  5.11s/it]\u001b[A\n",
      " 20%|██        | 179/889 [12:06<53:40,  4.54s/it]  \u001b[A\n",
      " 20%|██        | 182/889 [12:12<37:43,  3.20s/it]\u001b[A\n",
      " 21%|██        | 183/889 [12:19<46:13,  3.93s/it]\u001b[A\n",
      " 21%|██        | 184/889 [12:23<45:36,  3.88s/it]\u001b[A\n",
      " 21%|██        | 185/889 [12:27<46:50,  3.99s/it]\u001b[A\n",
      " 21%|██        | 186/889 [12:33<52:30,  4.48s/it]\u001b[A\n",
      " 21%|██        | 188/889 [12:39<44:12,  3.78s/it]\u001b[A\n",
      " 21%|██▏       | 191/889 [12:43<30:34,  2.63s/it]\u001b[A\n",
      " 22%|██▏       | 193/889 [12:48<30:22,  2.62s/it]\u001b[A\n",
      " 22%|██▏       | 194/889 [12:55<40:42,  3.51s/it]\u001b[A\n",
      " 22%|██▏       | 195/889 [13:00<43:09,  3.73s/it]\u001b[A\n",
      " 22%|██▏       | 197/889 [13:04<36:14,  3.14s/it]\u001b[A\n",
      " 22%|██▏       | 198/889 [13:07<36:15,  3.15s/it]\u001b[A\n",
      " 22%|██▏       | 199/889 [13:11<38:06,  3.31s/it]\u001b[A\n",
      " 22%|██▏       | 200/889 [13:15<38:40,  3.37s/it]\u001b[A\n",
      " 23%|██▎       | 201/889 [13:22<50:05,  4.37s/it]\u001b[A\n",
      " 23%|██▎       | 202/889 [13:26<47:55,  4.19s/it]\u001b[A\n",
      " 23%|██▎       | 203/889 [13:29<46:16,  4.05s/it]\u001b[A\n",
      " 23%|██▎       | 204/889 [13:34<47:28,  4.16s/it]\u001b[A\n",
      " 23%|██▎       | 206/889 [13:39<38:15,  3.36s/it]\u001b[A\n",
      " 23%|██▎       | 208/889 [13:43<33:36,  2.96s/it]\u001b[A\n",
      " 24%|██▎       | 210/889 [13:47<29:12,  2.58s/it]\u001b[A\n",
      " 24%|██▎       | 211/889 [13:53<37:40,  3.33s/it]\u001b[A\n",
      " 24%|██▍       | 212/889 [13:57<38:24,  3.40s/it]\u001b[A\n",
      " 24%|██▍       | 213/889 [14:04<48:26,  4.30s/it]\u001b[A\n",
      " 24%|██▍       | 214/889 [14:12<58:51,  5.23s/it]\u001b[A\n",
      " 24%|██▍       | 215/889 [14:15<52:48,  4.70s/it]\u001b[A\n",
      " 24%|██▍       | 216/889 [14:21<57:05,  5.09s/it]\u001b[A\n",
      " 24%|██▍       | 217/889 [14:24<48:54,  4.37s/it]\u001b[A\n",
      " 25%|██▍       | 218/889 [14:28<49:29,  4.43s/it]\u001b[A\n",
      " 25%|██▍       | 221/889 [14:34<32:43,  2.94s/it]\u001b[A\n",
      " 25%|██▍       | 222/889 [14:37<33:32,  3.02s/it]\u001b[A\n",
      " 25%|██▌       | 223/889 [14:41<35:54,  3.24s/it]\u001b[A\n",
      " 25%|██▌       | 224/889 [14:45<39:06,  3.53s/it]\u001b[A\n",
      " 25%|██▌       | 225/889 [14:49<38:27,  3.47s/it]\u001b[A\n",
      " 25%|██▌       | 226/889 [14:53<40:08,  3.63s/it]\u001b[A\n",
      " 26%|██▌       | 227/889 [14:56<38:51,  3.52s/it]\u001b[A\n",
      " 26%|██▌       | 229/889 [15:01<33:27,  3.04s/it]\u001b[A\n",
      " 26%|██▌       | 230/889 [15:04<34:46,  3.17s/it]\u001b[A\n",
      " 26%|██▌       | 232/889 [15:09<31:52,  2.91s/it]\u001b[A\n",
      " 27%|██▋       | 236/889 [15:13<19:34,  1.80s/it]\u001b[A\n",
      " 27%|██▋       | 237/889 [15:21<31:04,  2.86s/it]\u001b[A\n",
      " 27%|██▋       | 238/889 [15:24<31:31,  2.90s/it]\u001b[A\n",
      " 27%|██▋       | 239/889 [15:29<34:58,  3.23s/it]\u001b[A\n",
      " 27%|██▋       | 241/889 [15:35<34:19,  3.18s/it]\u001b[A\n",
      " 27%|██▋       | 243/889 [15:39<30:19,  2.82s/it]\u001b[A\n",
      " 27%|██▋       | 244/889 [15:43<32:16,  3.00s/it]\u001b[A\n",
      " 28%|██▊       | 245/889 [15:46<31:33,  2.94s/it]\u001b[A\n",
      " 28%|██▊       | 246/889 [15:50<35:10,  3.28s/it]\u001b[A\n",
      " 28%|██▊       | 247/889 [15:55<40:50,  3.82s/it]\u001b[A\n",
      " 28%|██▊       | 248/889 [15:59<41:16,  3.86s/it]\u001b[A\n",
      " 28%|██▊       | 249/889 [16:02<37:49,  3.55s/it]\u001b[A\n",
      " 28%|██▊       | 252/889 [16:10<32:43,  3.08s/it]\u001b[A\n",
      " 28%|██▊       | 253/889 [16:16<39:07,  3.69s/it]\u001b[A\n",
      " 29%|██▊       | 254/889 [16:20<38:29,  3.64s/it]\u001b[A\n",
      " 29%|██▊       | 255/889 [16:24<39:44,  3.76s/it]\u001b[A\n",
      " 29%|██▉       | 257/889 [16:27<30:27,  2.89s/it]\u001b[A\n",
      " 29%|██▉       | 258/889 [16:30<30:52,  2.94s/it]\u001b[A\n",
      " 29%|██▉       | 261/889 [16:36<24:40,  2.36s/it]\u001b[A\n",
      " 30%|██▉       | 264/889 [16:43<25:40,  2.46s/it]\u001b[A\n",
      " 30%|██▉       | 266/889 [16:48<24:28,  2.36s/it]\u001b[A\n",
      " 30%|███       | 267/889 [16:52<28:25,  2.74s/it]\u001b[A\n",
      " 30%|███       | 268/889 [16:58<34:59,  3.38s/it]\u001b[A\n",
      " 30%|███       | 269/889 [17:05<41:32,  4.02s/it]\u001b[A\n",
      " 30%|███       | 270/889 [17:09<42:54,  4.16s/it]\u001b[A\n",
      " 31%|███       | 272/889 [17:14<35:04,  3.41s/it]\u001b[A\n",
      " 31%|███       | 273/889 [17:18<36:29,  3.55s/it]\u001b[A\n",
      " 31%|███       | 274/889 [17:18<27:57,  2.73s/it]\u001b[A\n",
      " 31%|███       | 275/889 [17:22<31:26,  3.07s/it]\u001b[A\n",
      " 31%|███       | 276/889 [17:27<35:30,  3.48s/it]\u001b[A\n",
      " 31%|███▏      | 280/889 [17:32<21:52,  2.16s/it]\u001b[A\n",
      " 32%|███▏      | 281/889 [17:35<23:40,  2.34s/it]\u001b[A\n",
      " 32%|███▏      | 282/889 [17:39<27:07,  2.68s/it]\u001b[A\n",
      " 32%|███▏      | 285/889 [17:46<25:28,  2.53s/it]\u001b[A\n",
      " 32%|███▏      | 288/889 [17:52<23:07,  2.31s/it]\u001b[A\n",
      " 33%|███▎      | 289/889 [17:58<29:19,  2.93s/it]\u001b[A\n",
      " 33%|███▎      | 291/889 [18:06<32:10,  3.23s/it]\u001b[A\n",
      " 33%|███▎      | 292/889 [18:09<31:57,  3.21s/it]\u001b[A\n",
      " 33%|███▎      | 293/889 [18:17<40:51,  4.11s/it]\u001b[A\n",
      " 33%|███▎      | 294/889 [18:22<44:01,  4.44s/it]\u001b[A\n",
      " 33%|███▎      | 295/889 [18:31<54:52,  5.54s/it]\u001b[A\n",
      " 33%|███▎      | 296/889 [18:37<56:20,  5.70s/it]\u001b[A\n",
      " 34%|███▎      | 298/889 [18:43<43:37,  4.43s/it]\u001b[A\n",
      " 34%|███▎      | 299/889 [18:51<52:18,  5.32s/it]\u001b[A\n",
      " 34%|███▎      | 300/889 [18:55<50:08,  5.11s/it]\u001b[A\n",
      " 34%|███▍      | 301/889 [18:59<45:08,  4.61s/it]\u001b[A\n",
      " 34%|███▍      | 302/889 [19:03<43:55,  4.49s/it]\u001b[A\n",
      " 34%|███▍      | 304/889 [19:07<34:14,  3.51s/it]\u001b[A\n",
      " 35%|███▍      | 307/889 [19:11<23:47,  2.45s/it]\u001b[A\n",
      " 35%|███▍      | 308/889 [19:15<25:31,  2.64s/it]\u001b[A\n",
      " 35%|███▍      | 309/889 [19:22<34:43,  3.59s/it]\u001b[A\n",
      " 35%|███▍      | 310/889 [19:27<37:47,  3.92s/it]\u001b[A\n",
      " 35%|███▍      | 311/889 [19:31<37:37,  3.91s/it]\u001b[A\n",
      " 35%|███▌      | 312/889 [19:36<39:56,  4.15s/it]\u001b[A\n",
      " 35%|███▌      | 313/889 [19:40<41:18,  4.30s/it]\u001b[A\n",
      " 35%|███▌      | 314/889 [19:43<37:12,  3.88s/it]\u001b[A\n",
      " 36%|███▌      | 316/889 [19:51<36:20,  3.81s/it]\u001b[A\n",
      " 36%|███▌      | 318/889 [19:55<31:00,  3.26s/it]\u001b[A\n",
      " 36%|███▌      | 320/889 [19:58<24:23,  2.57s/it]\u001b[A\n",
      " 36%|███▌      | 321/889 [20:02<26:28,  2.80s/it]\u001b[A\n",
      " 36%|███▌      | 322/889 [20:07<32:25,  3.43s/it]\u001b[A\n",
      " 36%|███▋      | 323/889 [20:09<28:45,  3.05s/it]\u001b[A\n",
      " 36%|███▋      | 324/889 [20:13<31:05,  3.30s/it]\u001b[A\n",
      " 37%|███▋      | 326/889 [20:18<26:56,  2.87s/it]\u001b[A\n",
      " 37%|███▋      | 327/889 [20:24<34:12,  3.65s/it]\u001b[A\n",
      " 37%|███▋      | 328/889 [20:29<36:07,  3.86s/it]\u001b[A\n",
      " 37%|███▋      | 329/889 [20:33<37:47,  4.05s/it]\u001b[A\n",
      " 37%|███▋      | 330/889 [20:37<36:49,  3.95s/it]\u001b[A\n",
      " 37%|███▋      | 332/889 [20:41<28:46,  3.10s/it]\u001b[A\n",
      " 38%|███▊      | 335/889 [20:45<20:39,  2.24s/it]\u001b[A\n",
      " 38%|███▊      | 338/889 [20:54<23:16,  2.54s/it]\u001b[A\n",
      " 38%|███▊      | 340/889 [20:57<21:36,  2.36s/it]\u001b[A\n",
      " 38%|███▊      | 342/889 [21:03<23:01,  2.52s/it]\u001b[A\n",
      " 39%|███▊      | 343/889 [21:07<25:10,  2.77s/it]\u001b[A\n",
      " 39%|███▊      | 344/889 [21:12<28:23,  3.13s/it]\u001b[A\n",
      " 39%|███▉      | 345/889 [21:16<29:58,  3.31s/it]\u001b[A\n",
      " 39%|███▉      | 347/889 [21:21<27:50,  3.08s/it]\u001b[A\n",
      " 39%|███▉      | 348/889 [21:29<37:12,  4.13s/it]\u001b[A\n",
      " 39%|███▉      | 349/889 [21:33<36:45,  4.08s/it]\u001b[A\n",
      " 39%|███▉      | 350/889 [21:40<43:01,  4.79s/it]\u001b[A\n",
      " 39%|███▉      | 351/889 [21:44<41:44,  4.66s/it]\u001b[A\n",
      " 40%|███▉      | 353/889 [21:50<34:34,  3.87s/it]\u001b[A\n",
      " 40%|███▉      | 354/889 [21:54<34:40,  3.89s/it]\u001b[A\n",
      " 40%|███▉      | 355/889 [21:56<31:05,  3.49s/it]\u001b[A\n",
      " 40%|████      | 356/889 [22:02<36:14,  4.08s/it]\u001b[A\n",
      " 40%|████      | 357/889 [22:06<36:46,  4.15s/it]\u001b[A\n",
      " 40%|████      | 358/889 [22:11<37:04,  4.19s/it]\u001b[A\n",
      " 40%|████      | 360/889 [22:15<29:51,  3.39s/it]\u001b[A\n",
      " 41%|████      | 361/889 [22:20<32:28,  3.69s/it]\u001b[A\n",
      " 41%|████      | 362/889 [22:24<32:24,  3.69s/it]\u001b[A\n",
      " 41%|████      | 363/889 [22:29<36:29,  4.16s/it]\u001b[A\n",
      " 41%|████      | 364/889 [22:34<37:51,  4.33s/it]\u001b[A\n",
      " 41%|████      | 365/889 [22:40<41:03,  4.70s/it]\u001b[A\n",
      " 41%|████▏     | 368/889 [22:43<24:29,  2.82s/it]\u001b[A\n",
      " 42%|████▏     | 370/889 [22:50<25:17,  2.92s/it]\u001b[A\n",
      " 42%|████▏     | 371/889 [22:54<27:38,  3.20s/it]\u001b[A\n",
      " 42%|████▏     | 374/889 [22:58<20:12,  2.36s/it]\u001b[A\n",
      " 42%|████▏     | 375/889 [23:02<22:49,  2.66s/it]\u001b[A\n",
      " 42%|████▏     | 376/889 [23:07<27:24,  3.20s/it]\u001b[A\n",
      " 42%|████▏     | 377/889 [23:14<33:24,  3.91s/it]\u001b[A\n",
      " 43%|████▎     | 378/889 [23:18<33:13,  3.90s/it]\u001b[A\n",
      " 43%|████▎     | 379/889 [23:21<31:01,  3.65s/it]\u001b[A\n",
      " 43%|████▎     | 380/889 [23:26<33:53,  4.00s/it]\u001b[A\n",
      " 43%|████▎     | 384/889 [23:28<16:49,  2.00s/it]\u001b[A\n",
      " 43%|████▎     | 385/889 [23:34<21:45,  2.59s/it]\u001b[A\n",
      " 43%|████▎     | 386/889 [23:39<25:38,  3.06s/it]\u001b[A\n",
      " 44%|████▎     | 387/889 [23:41<24:46,  2.96s/it]\u001b[A\n",
      " 44%|████▎     | 388/889 [23:46<28:34,  3.42s/it]\u001b[A\n",
      " 44%|████▍     | 389/889 [23:50<29:46,  3.57s/it]\u001b[A\n",
      " 44%|████▍     | 390/889 [23:53<27:28,  3.30s/it]\u001b[A\n",
      " 44%|████▍     | 391/889 [23:59<34:59,  4.22s/it]\u001b[A\n",
      " 44%|████▍     | 392/889 [24:06<41:58,  5.07s/it]\u001b[A\n",
      " 44%|████▍     | 394/889 [24:09<28:15,  3.42s/it]\u001b[A\n",
      " 44%|████▍     | 395/889 [24:15<32:51,  3.99s/it]\u001b[A\n",
      " 45%|████▍     | 397/889 [24:18<24:00,  2.93s/it]\u001b[A\n",
      " 45%|████▍     | 399/889 [24:21<20:20,  2.49s/it]\u001b[A\n",
      " 45%|████▍     | 400/889 [24:23<19:30,  2.39s/it]\u001b[A\n",
      " 45%|████▌     | 402/889 [24:28<19:14,  2.37s/it]\u001b[A\n",
      " 45%|████▌     | 403/889 [24:34<25:06,  3.10s/it]\u001b[A\n",
      " 45%|████▌     | 404/889 [24:38<26:21,  3.26s/it]\u001b[A\n",
      " 46%|████▌     | 405/889 [24:40<25:10,  3.12s/it]\u001b[A\n",
      " 46%|████▌     | 407/889 [24:44<21:06,  2.63s/it]\u001b[A\n",
      " 46%|████▌     | 409/889 [24:50<21:31,  2.69s/it]\u001b[A\n",
      " 46%|████▌     | 410/889 [24:53<23:00,  2.88s/it]\u001b[A\n",
      " 46%|████▌     | 411/889 [24:56<22:09,  2.78s/it]\u001b[A\n",
      " 46%|████▋     | 413/889 [24:58<16:26,  2.07s/it]\u001b[A\n",
      " 47%|████▋     | 414/889 [25:00<17:04,  2.16s/it]\u001b[A\n",
      " 47%|████▋     | 415/889 [25:04<19:59,  2.53s/it]\u001b[A\n",
      " 47%|████▋     | 416/889 [25:09<24:34,  3.12s/it]\u001b[A\n",
      " 47%|████▋     | 417/889 [25:13<26:40,  3.39s/it]\u001b[A\n",
      " 47%|████▋     | 418/889 [25:20<34:36,  4.41s/it]\u001b[A\n",
      " 47%|████▋     | 419/889 [25:23<31:40,  4.04s/it]\u001b[A\n",
      " 47%|████▋     | 420/889 [25:28<34:39,  4.43s/it]\u001b[A\n",
      " 47%|████▋     | 421/889 [25:32<32:10,  4.13s/it]\u001b[A\n",
      " 48%|████▊     | 424/889 [25:37<21:39,  2.80s/it]\u001b[A\n",
      " 48%|████▊     | 426/889 [25:41<18:53,  2.45s/it]\u001b[A\n",
      " 48%|████▊     | 427/889 [25:47<24:20,  3.16s/it]\u001b[A\n",
      " 48%|████▊     | 429/889 [25:51<21:56,  2.86s/it]\u001b[A\n",
      " 48%|████▊     | 431/889 [25:57<21:08,  2.77s/it]\u001b[A\n",
      " 49%|████▊     | 432/889 [25:58<19:46,  2.60s/it]\u001b[A\n",
      " 49%|████▊     | 433/889 [26:01<19:29,  2.57s/it]\u001b[A\n",
      " 49%|████▉     | 436/889 [26:04<13:28,  1.79s/it]\u001b[A\n",
      " 49%|████▉     | 437/889 [26:08<17:11,  2.28s/it]\u001b[A\n",
      " 49%|████▉     | 438/889 [26:12<19:35,  2.61s/it]\u001b[A\n",
      " 49%|████▉     | 439/889 [26:16<21:39,  2.89s/it]\u001b[A\n",
      " 49%|████▉     | 440/889 [26:20<24:17,  3.25s/it]\u001b[A\n",
      " 50%|████▉     | 441/889 [26:22<20:55,  2.80s/it]\u001b[A\n",
      " 50%|████▉     | 443/889 [26:26<18:18,  2.46s/it]\u001b[A\n",
      " 50%|████▉     | 444/889 [26:30<21:39,  2.92s/it]\u001b[A\n",
      " 50%|█████     | 446/889 [26:33<17:45,  2.40s/it]\u001b[A\n",
      " 50%|█████     | 447/889 [26:37<19:48,  2.69s/it]\u001b[A\n",
      " 50%|█████     | 448/889 [26:41<21:35,  2.94s/it]\u001b[A\n",
      " 51%|█████     | 449/889 [26:44<22:57,  3.13s/it]\u001b[A\n",
      " 51%|█████     | 450/889 [26:47<20:51,  2.85s/it]\u001b[A\n",
      " 51%|█████     | 453/889 [26:53<17:48,  2.45s/it]\u001b[A\n",
      " 51%|█████     | 454/889 [26:56<19:22,  2.67s/it]\u001b[A\n",
      " 51%|█████     | 455/889 [27:00<21:30,  2.97s/it]\u001b[A\n",
      " 51%|█████▏    | 456/889 [27:05<24:52,  3.45s/it]\u001b[A\n",
      " 51%|█████▏    | 457/889 [27:13<31:59,  4.44s/it]\u001b[A\n",
      " 52%|█████▏    | 458/889 [27:14<26:18,  3.66s/it]\u001b[A\n",
      " 52%|█████▏    | 459/889 [27:17<24:22,  3.40s/it]\u001b[A\n",
      " 52%|█████▏    | 460/889 [27:22<27:56,  3.91s/it]\u001b[A\n",
      " 52%|█████▏    | 461/889 [27:26<27:41,  3.88s/it]\u001b[A\n",
      " 52%|█████▏    | 462/889 [27:28<24:28,  3.44s/it]\u001b[A\n",
      " 52%|█████▏    | 463/889 [27:35<31:51,  4.49s/it]\u001b[A\n",
      " 52%|█████▏    | 464/889 [27:38<28:06,  3.97s/it]\u001b[A\n",
      " 52%|█████▏    | 466/889 [27:41<20:21,  2.89s/it]\u001b[A\n",
      " 53%|█████▎    | 467/889 [27:46<22:59,  3.27s/it]\u001b[A\n",
      " 53%|█████▎    | 468/889 [27:52<29:24,  4.19s/it]\u001b[A\n",
      " 53%|█████▎    | 469/889 [27:57<30:16,  4.32s/it]\u001b[A\n",
      " 53%|█████▎    | 470/889 [28:01<29:13,  4.19s/it]\u001b[A\n",
      " 53%|█████▎    | 471/889 [28:08<34:31,  4.96s/it]\u001b[A\n",
      " 53%|█████▎    | 472/889 [28:12<31:55,  4.59s/it]\u001b[A\n",
      " 53%|█████▎    | 475/889 [28:14<17:09,  2.49s/it]\u001b[A\n",
      " 54%|█████▎    | 476/889 [28:18<19:13,  2.79s/it]\u001b[A\n",
      " 54%|█████▎    | 477/889 [28:22<21:06,  3.07s/it]\u001b[A\n",
      " 54%|█████▍    | 478/889 [28:26<22:20,  3.26s/it]\u001b[A\n",
      " 54%|█████▍    | 479/889 [28:31<25:57,  3.80s/it]\u001b[A\n",
      " 54%|█████▍    | 480/889 [28:34<24:31,  3.60s/it]\u001b[A\n",
      " 54%|█████▍    | 481/889 [28:38<24:33,  3.61s/it]\u001b[A\n",
      " 54%|█████▍    | 482/889 [28:42<25:39,  3.78s/it]\u001b[A\n",
      " 54%|█████▍    | 483/889 [28:46<25:13,  3.73s/it]\u001b[A\n",
      " 54%|█████▍    | 484/889 [28:50<27:25,  4.06s/it]\u001b[A\n",
      " 55%|█████▍    | 485/889 [28:54<26:23,  3.92s/it]\u001b[A\n",
      " 55%|█████▍    | 486/889 [29:01<32:07,  4.78s/it]\u001b[A\n",
      " 55%|█████▍    | 487/889 [29:05<30:32,  4.56s/it]\u001b[A\n",
      " 55%|█████▍    | 488/889 [29:09<29:17,  4.38s/it]\u001b[A\n",
      " 55%|█████▌    | 489/889 [29:11<25:39,  3.85s/it]\u001b[A\n",
      " 55%|█████▌    | 491/889 [29:14<17:34,  2.65s/it]\u001b[A\n",
      " 55%|█████▌    | 492/889 [29:20<23:03,  3.48s/it]\u001b[A\n",
      " 55%|█████▌    | 493/889 [29:25<25:22,  3.84s/it]\u001b[A\n",
      " 56%|█████▌    | 494/889 [29:28<23:32,  3.58s/it]\u001b[A\n",
      " 56%|█████▌    | 495/889 [29:32<24:53,  3.79s/it]\u001b[A\n",
      " 56%|█████▌    | 496/889 [29:36<25:26,  3.88s/it]\u001b[A\n",
      " 56%|█████▌    | 497/889 [29:40<25:04,  3.84s/it]\u001b[A\n",
      " 56%|█████▌    | 499/889 [29:42<16:53,  2.60s/it]\u001b[A\n",
      " 56%|█████▌    | 500/889 [29:46<18:39,  2.88s/it]\u001b[A\n",
      " 56%|█████▋    | 501/889 [29:49<18:52,  2.92s/it]\u001b[A\n",
      " 56%|█████▋    | 502/889 [29:54<22:38,  3.51s/it]\u001b[A\n",
      " 57%|█████▋    | 503/889 [29:58<23:32,  3.66s/it]\u001b[A\n",
      " 57%|█████▋    | 504/889 [30:02<23:42,  3.69s/it]\u001b[A\n",
      " 57%|█████▋    | 505/889 [30:07<26:03,  4.07s/it]\u001b[A\n",
      " 57%|█████▋    | 506/889 [30:15<33:57,  5.32s/it]\u001b[A\n",
      " 57%|█████▋    | 508/889 [30:21<26:30,  4.17s/it]\u001b[A\n",
      " 57%|█████▋    | 509/889 [30:25<26:33,  4.19s/it]\u001b[A\n",
      " 57%|█████▋    | 510/889 [30:31<30:12,  4.78s/it]\u001b[A\n",
      " 57%|█████▋    | 511/889 [30:34<27:00,  4.29s/it]\u001b[A\n",
      " 58%|█████▊    | 512/889 [30:39<26:58,  4.29s/it]\u001b[A\n",
      " 58%|█████▊    | 513/889 [30:45<29:49,  4.76s/it]\u001b[A\n",
      " 58%|█████▊    | 514/889 [30:49<28:23,  4.54s/it]\u001b[A\n",
      " 58%|█████▊    | 515/889 [30:58<36:14,  5.81s/it]\u001b[A\n",
      " 58%|█████▊    | 516/889 [31:07<43:13,  6.95s/it]\u001b[A\n",
      " 58%|█████▊    | 517/889 [31:12<39:30,  6.37s/it]\u001b[A\n",
      " 58%|█████▊    | 518/889 [31:18<37:29,  6.06s/it]\u001b[A\n",
      " 58%|█████▊    | 519/889 [31:22<33:36,  5.45s/it]\u001b[A\n",
      " 59%|█████▊    | 521/889 [31:27<25:05,  4.09s/it]\u001b[A\n",
      " 59%|█████▊    | 522/889 [31:31<25:16,  4.13s/it]\u001b[A\n",
      " 59%|█████▉    | 523/889 [31:35<26:04,  4.28s/it]\u001b[A\n",
      " 59%|█████▉    | 524/889 [31:39<24:46,  4.07s/it]\u001b[A\n",
      " 59%|█████▉    | 528/889 [31:44<21:41,  3.61s/it]\u001b[A\n",
      "  0%|          | 0/1 [31:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j0/d6kb3mqd2rdcd0jvfxnzslxw0000gn/T/ipykernel_52044/196803709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# Calculate time series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mtime_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmri_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mn_regions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_time_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nilearn/maskers/nifti_labels_masker.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, imgs, confounds, sample_mask)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m         return self.fit().transform(imgs, confounds=confounds,\n\u001b[0m\u001b[1;32m    456\u001b[0m                                     sample_mask=sample_mask)\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nilearn/maskers/base_masker.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, imgs, confounds, sample_mask)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfounds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh_variance_confounds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             return self.transform_single_imgs(imgs,\n\u001b[0m\u001b[1;32m    233\u001b[0m                                               \u001b[0mconfounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                                               sample_mask=sample_mask)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nilearn/maskers/nifti_labels_masker.py\u001b[0m in \u001b[0;36mtransform_single_imgs\u001b[0;34m(self, imgs, confounds, sample_mask)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresampling_target\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mimgs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_niimg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matleast_4d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             if not _utils.niimg_conversions._check_same_fov(\n\u001b[1;32m    517\u001b[0m                 \u001b[0mimgs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nilearn/_utils/niimg_conversions.py\u001b[0m in \u001b[0;36mcheck_niimg\u001b[0;34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;31m# Otherwise, it should be a filename or a SpatialImage, we load it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mniimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_niimg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_ndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mniimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nilearn/_utils/niimg.py\u001b[0m in \u001b[0;36mload_niimg\u001b[0;34m(niimg, dtype)\u001b[0m\n\u001b[1;32m    132\u001b[0m         )\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_target_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nilearn/_utils/niimg.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nibabel/arrayproxy.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mScaled\u001b[0m \u001b[0mimage\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_scaled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslicer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nibabel/arrayproxy.py\u001b[0m in \u001b[0;36m_get_scaled\u001b[0;34m(self, dtype, slicer)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mscl_inter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscl_inter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m# Read array and upcast as necessary for big slopes, intercepts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_read_scaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_unscaled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscl_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscl_inter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpromote_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nibabel/arrayproxy.py\u001b[0m in \u001b[0;36m_get_unscaled\u001b[0;34m(self, slicer)\u001b[0m\n\u001b[1;32m    374\u001b[0m         ):\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_fileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 return array_from_file(\n\u001b[0m\u001b[1;32m    377\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nibabel/volumeutils.py\u001b[0m in \u001b[0;36marray_from_file\u001b[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'readinto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mdata_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mn_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0mneeds_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;31m# Read a chunk of data from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0muncompress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import input_data, datasets\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from fastdtw import fastdtw\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.nn import BatchNorm1d, Dropout\n",
    "import os\n",
    "from nilearn import plotting\n",
    "from nilearn import image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pprint\n",
    "# Ignore all warnings (not recommended unless you know what you are doing)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "train_data_eda = \"cort-maxprob-thr25-2mm_0_3_cor_train_eda/\"\n",
    "os.makedirs(train_data_eda, exist_ok=True)\n",
    "\n",
    "test_result_dir = \"cort-maxprob-thr25-2mm_0_3_cor_test_result/\"\n",
    "os.makedirs(test_result_dir, exist_ok=True)\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = pd.read_csv(r\"/Users/vinoth/PycharmProjects/paper_implementation/Dataset/source/mri_images/ABIDE_pcp/Phenotypic_V1_0b_preprocessed1.csv\")\n",
    "csv_file['DX_GROUP'].replace({1: 0, 2: 1}, inplace=True)\n",
    "train_df, test_df = train_test_split(csv_file, test_size=0.2, random_state=42)\n",
    "harvard_oxford_atlas = ['cort-maxprob-thr25-2mm']\n",
    "'''values = [\n",
    "    \"cort-maxprob-thr0-1mm\",\n",
    "    \"cort-maxprob-thr0-2mm\",\n",
    "    \"cort-maxprob-thr25-1mm\",\n",
    "    \"cort-maxprob-thr25-2mm\",\n",
    "    \"cort-maxprob-thr50-1mm\",\n",
    "    \"cort-maxprob-thr50-2mm\",\n",
    "    \"cort-prob-1mm\",\n",
    "    \"cort-prob-2mm\",\n",
    "    \"cortl-maxprob-thr0-1mm\",\n",
    "    \"cortl-maxprob-thr0-2mm\",\n",
    "    \"cortl-maxprob-thr25-1mm\",\n",
    "    \"cortl-maxprob-thr25-2mm\",\n",
    "    \"cortl-maxprob-thr50-1mm\",\n",
    "    \"cortl-maxprob-thr50-2mm\",\n",
    "    \"cortl-prob-1mm\",\n",
    "    \"cortl-prob-2mm\",\n",
    "    \"sub-maxprob-thr0-1mm\",\n",
    "    \"sub-maxprob-thr0-2mm\",\n",
    "    \"sub-maxprob-thr25-1mm\",\n",
    "    \"sub-maxprob-thr25-2mm\",\n",
    "    \"sub-maxprob-thr50-1mm\",\n",
    "    \"sub-maxprob-thr50-2mm\",\n",
    "    \"sub-prob-1mm\",\n",
    "    \"sub-prob-2mm\"\n",
    "]'''\n",
    "\n",
    "\n",
    "results = {}\n",
    "atlas_threshold = None\n",
    "\n",
    "for data in tqdm(harvard_oxford_atlas):\n",
    "    atlas_threshold = data\n",
    "    print(\"----Threshold----\")\n",
    "    print(atlas_threshold)\n",
    "    results[atlas_threshold] = {}\n",
    "    atlas = datasets.fetch_atlas_harvard_oxford(data)\n",
    "    masker = input_data.NiftiLabelsMasker(labels_img=atlas.maps, standardize=True)\n",
    "    mri_dir = r\"/Users/vinoth/PycharmProjects/paper_implementation/Dataset/source/mri_images/ABIDE_pcp/cpac/nofilt_noglobal/\"\n",
    "\n",
    "    # Placeholder for Graph Neural Network Data\n",
    "    graph_data_list = []\n",
    "\n",
    "    # Data Preprocessing\n",
    "    for idx, row in tqdm(enumerate(train_df.itertuples()), total=len(train_df)):\n",
    "        # Combine the parent and nested folder paths\n",
    "        file_dir = os.path.join(train_data_eda, row.FILE_ID)\n",
    "        os.makedirs(file_dir, exist_ok=True)\n",
    "        mri_filename = os.path.join(mri_dir, row.FILE_ID + \"_func_preproc.nii.gz\")\n",
    "        \n",
    "        try:\n",
    "            mri_img = nib.load(mri_filename)\n",
    "            \n",
    "            mri_img_dir = os.path.join(file_dir, 'mri_image')\n",
    "            os.makedirs(mri_img_dir, exist_ok=True)\n",
    "            \n",
    "            # Select the first time point\n",
    "            first_volume = mri_img.slicer[:,:,:,0]\n",
    "            \n",
    "            image_shape = mri_img.shape\n",
    "\n",
    "            # The total number of volumes in the 4D dimension is the size of the fourth dimension\n",
    "            total_volumes = image_shape[3]\n",
    "\n",
    "            #print(\"Total number of volumes in the 4D image for file \" + row.FILE_ID + \" : \", total_volumes)\n",
    "\n",
    "            '''# Plot the image\n",
    "            plotting.plot_img(first_volume, cmap='gray')  # grayscale often works well for MRIs\n",
    "            filename = os.path.join(mri_img_dir, row.FILE_ID+'_img.png')\n",
    "            plt.savefig(filename)\n",
    "            plt.close()  # Close the plot to avoid overlaps\n",
    "\n",
    "            # Plot the EPI\n",
    "            plotting.plot_epi(first_volume, display_mode='z', cut_coords=5, cmap='viridis')  # viridis is a perceptually uniform colormap\n",
    "            filename = os.path.join(mri_img_dir, row.FILE_ID+'_epi_img.png')\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "\n",
    "            # Plot the anatomy\n",
    "            plotting.plot_anat(first_volume, cmap='gray')  # grayscale again for anatomical images\n",
    "            filename = os.path.join(mri_img_dir, row.FILE_ID+'_anat_img.png')\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "\n",
    "            # Plot the statistical map\n",
    "            plotting.plot_stat_map(first_volume, bg_img=None, threshold=3.0, cmap='cold_hot')  # cold_hot is often used for stat maps\n",
    "            filename = os.path.join(mri_img_dir, row.FILE_ID+'_stat_map_img.png')\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "\n",
    "            # Plot the probabilistic atlas\n",
    "            plotting.plot_prob_atlas(mri_img, bg_img=None, colorbar=True)  # default colormap should work for probabilistic atlas\n",
    "            filename = os.path.join(mri_img_dir, row.FILE_ID+'_atlas_map_img.png')\n",
    "            plt.savefig(filename)\n",
    "            plt.close()'''\n",
    "            \n",
    "            # Calculate time series\n",
    "            time_series = masker.fit_transform(mri_img)\n",
    "\n",
    "            n_regions, n_time_points = time_series.shape\n",
    "            number_of_entries = n_regions * n_time_points\n",
    "\n",
    "            # Create an adjacency matrix based on ROI correlations\n",
    "            correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "            correlation_matrix = correlation_measure.fit_transform([time_series])[0]\n",
    "\n",
    "            # Threshold the correlation matrix (optional)\n",
    "            threshold = 0.3  # Set the threshold value for the correlation\n",
    "            correlation_matrix[abs(correlation_matrix) < threshold] = 0\n",
    "\n",
    "            # Generate graph from correlation matrix\n",
    "            G = nx.from_numpy_matrix(correlation_matrix)\n",
    "            \n",
    "\n",
    "            '''if idx == 0:  # Only for the first iteration\n",
    "                # Plot the time series for the regions\n",
    "                plt.figure(figsize=(35, 15))\n",
    "                for i in range(min(n_regions, time_series.shape[0])):\n",
    "                    plt.plot(time_series[i, :], label=f'Region {i + 1}')\n",
    "                plt.xlabel('Time point')\n",
    "                plt.ylabel('Blood Oxygen Level(BOLD) - Normalized signal')\n",
    "                plt.title('Time series of the regions')\n",
    "                plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "                plt.tight_layout()\n",
    "            \n",
    "                # Save the plot to the existing folder\n",
    "                time_series_filename = row.FILE_ID+'_'+'time_series_plot.png'\n",
    "                \n",
    "                plt.savefig(os.path.join(file_dir, time_series_filename))\n",
    "                \n",
    "                plt.figure(figsize=(10, 10))\n",
    "                sns.heatmap(similarity_matrix, annot=False, cmap='turbo')\n",
    "                plt.title('Similarity Matrix')\n",
    "                similarity_matrix_adj_img_filename = row.FILE_ID + '_similarity_matrix.png'\n",
    "                plt.savefig(os.path.join(file_dir, similarity_matrix_adj_img_filename))\n",
    "                plt.close() # Close the plot\n",
    "                similarity_matrix_npy_filename = row.FILE_ID + '_similarity_matrix.npy'\n",
    "                similarity_matrix_npy_path = os.path.join(file_dir, similarity_matrix_npy_filename)\n",
    "                np.save(similarity_matrix_npy_path, similarity_matrix)\n",
    "\n",
    "                # Visualize the graph\n",
    "                plt.figure(figsize=(45, 25))\n",
    "                pos = nx.spring_layout(G)\n",
    "\n",
    "                # Extract the edge weights from the graph\n",
    "                weights = [G[u][v].get('weight', 1) for u, v in G.edges()]\n",
    "\n",
    "                # Normalize the weights to fit your desired range of thickness\n",
    "                normalized_weights = [5 * weight / max(weights) for weight in weights]\n",
    "\n",
    "                # Draw the edges with the thickness determined by the normalized weights\n",
    "                nx.draw_networkx_edges(G, pos, width=normalized_weights)\n",
    "\n",
    "                # Draw the nodes and labels\n",
    "                nx.draw_networkx_nodes(G, pos)\n",
    "                nx.draw_networkx_labels(G, pos)\n",
    "                \n",
    "                # Define the path and filename where you want to save the plot\n",
    "                graph_plot_filename = row.FILE_ID + '_graph_plot.png'\n",
    "                graph_plot_path = os.path.join(file_dir, graph_plot_filename)\n",
    "\n",
    "                # Save the plot to the specified path\n",
    "                plt.savefig(graph_plot_path'''\n",
    "\n",
    "            edge_index = torch.tensor(list(G.edges), dtype=torch.long)\n",
    "            x = torch.tensor(time_series, dtype=torch.float)\n",
    "            y = torch.tensor([row.DX_GROUP], dtype=torch.float)\n",
    "            data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "            graph_data_list.append(data)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    # Neural Network Model with Regularization, Batch Normalization, and Dropout\n",
    "    class Net(torch.nn.Module):\n",
    "        def __init__(self, num_node_features, num_classes):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = GCNConv(num_node_features, 16)\n",
    "            self.bn1 = BatchNorm1d(16)\n",
    "            self.conv2 = GCNConv(16, 32)\n",
    "            self.bn2 = BatchNorm1d(32)\n",
    "            self.fc = torch.nn.Linear(32, num_classes)\n",
    "            self.dropout = Dropout(0.5)\n",
    "\n",
    "        def forward(self, data):\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = self.bn2(x)\n",
    "            x = global_mean_pool(x, batch)\n",
    "            x = self.fc(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_features = graph_data_list[0].num_node_features\n",
    "    num_classes = 2\n",
    "    model = Net(num_features, num_classes).to(device)\n",
    "    loader = DataLoader(graph_data_list, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Hyperparameter Tuning (Example: Adjusting Learning Rate)\n",
    "    learning_rates = [0.01, 0.001, 0.0001]\n",
    "    l_rate = None\n",
    "    for lr in learning_rates:\n",
    "        l_rate = str(lr)\n",
    "        results[atlas_threshold][l_rate] = {}\n",
    "        print(\"Learning Rate --> \", lr)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4) # L2 Regularization\n",
    "        for epoch in tqdm(range(100)):\n",
    "            total_loss = 0\n",
    "            model.train()\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data)\n",
    "                loss = F.nll_loss(out, data.y.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f'Epoch: {epoch+1}, Loss: {total_loss/len(loader)}')\n",
    "        results[atlas_threshold][l_rate]['loss'] = total_loss/len(loader)\n",
    "\n",
    "        # Placeholder for time series data\n",
    "        time_series_list = []\n",
    "        successful_indices = []\n",
    "\n",
    "        # Testing Data Preprocessing\n",
    "        # for idx, row in enumerate(test_df.itertuples()):\n",
    "        for idx, row in tqdm(enumerate(test_df.itertuples()), total=len(test_df)):\n",
    "            '''if idx == 2:\n",
    "                break'''\n",
    "            mri_filename = os.path.join(mri_dir, row.FILE_ID + \"_func_preproc.nii.gz\")\n",
    "            try:\n",
    "                mri_img = nib.load(mri_filename)\n",
    "                time_series = masker.fit_transform(mri_img)\n",
    "                time_series_list.append(time_series)\n",
    "                successful_indices.append(idx)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "        \n",
    "        # Placeholder for Graph Neural Network Data for testing\n",
    "        graph_data_test_list = []\n",
    "\n",
    "        #for idx, successful_idx in tqdm(enumerate(successful_indices, total=len(successful_indices))):\n",
    "        for idx, successful_idx in tqdm(enumerate(successful_indices), total=len(successful_indices)):\n",
    "            row = test_df.iloc[successful_idx]\n",
    "            time_series = time_series_list[idx]\n",
    "            # Calculate time series\n",
    "            #time_series = masker.fit_transform(mri_img)\n",
    "\n",
    "            # Create an adjacency matrix based on ROI correlations\n",
    "            correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "            correlation_matrix = correlation_measure.fit_transform([time_series])[0]\n",
    "\n",
    "            # Threshold the correlation matrix (optional)\n",
    "            threshold = 0.3  # Set the threshold value for the correlation\n",
    "            correlation_matrix[abs(correlation_matrix) < threshold] = 0\n",
    "\n",
    "            # Generate graph from correlation matrix\n",
    "            G = nx.from_numpy_matrix(correlation_matrix)\n",
    "            edge_index = torch.tensor(list(G.edges), dtype=torch.long)\n",
    "            x = torch.tensor(time_series, dtype=torch.float)\n",
    "            y = torch.tensor([row.DX_GROUP], dtype=torch.float)\n",
    "            data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "            graph_data_test_list.append(data)\n",
    "\n",
    "        # Create a data loader for testing data\n",
    "        test_loader = DataLoader(graph_data_test_list, batch_size=32, shuffle=False)\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for data in tqdm(test_loader):\n",
    "            data = data.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "                _, pred = output.max(dim=1)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_labels.append(data.y.cpu().numpy())\n",
    "            correct += int((pred == data.y.long()).sum())\n",
    "\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "        results[atlas_threshold][l_rate]['accuracy'] = accuracy\n",
    "\n",
    "        # Flatten the list of predictions and labels\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        \n",
    "        # Specify the parent folder\n",
    "        parent_folder = test_result_dir\n",
    "\n",
    "        # Specify the nested folder names\n",
    "        nested_folder1 = atlas_threshold\n",
    "        nested_folder2 = 'learning_rate_'+l_rate\n",
    "        \n",
    "        # Combine the parent and nested folder pa'hs\n",
    "        validation_result_dir = os.path.join(parent_folder, nested_folder1, nested_folder2)\n",
    "        \n",
    "        # Create the nested folders, including any necessary parent directories\n",
    "        os.makedirs(validation_result_dir, exist_ok=True)\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(cm, annot=True, cmap='Blues', fmt=\".2%\")\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix (Normalized)')\n",
    "        \n",
    "        # Save the image inside the nested folder\n",
    "        plt.savefig(os.path.join(validation_result_dir, 'confusion_matrix.png'))\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        # Print actual vs predicted\n",
    "        actual_vs_predicted = pd.DataFrame({'Actual': all_labels, 'Predicted': all_preds})\n",
    "        print(actual_vs_predicted)\n",
    "\n",
    "        # Classification report\n",
    "        report = classification_report(all_labels, all_preds, target_names=['Non-Autistic', 'Autistic'], output_dict=True)\n",
    "        #print(classification_report(all_labels, all_preds, target_names=['Non-Autistic', 'Autistic']))\n",
    "        report_text = classification_report(all_labels, all_preds, target_names=['Non-Autistic', 'Autistic'])\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.text(0.01, 0.05, report_text, {'fontsize': 12}, fontproperties='monospace') # Adjust text size and position accordingly\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(validation_result_dir, 'classification_report.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        with open(os.path.join(validation_result_dir, 'classification_report.txt'), 'w') as file:\n",
    "            file.write(report_text)\n",
    "  \n",
    "        # Individual class metrics\n",
    "        # Access individual values\n",
    "        non_autistic_precision = report['Non-Autistic']['precision']\n",
    "        autistic_precision = report['Autistic']['precision']\n",
    "\n",
    "        non_autistic_recall = report['Non-Autistic']['recall']\n",
    "        autistic_recall = report['Autistic']['recall']\n",
    "\n",
    "        non_autistic_f1_score = report['Non-Autistic']['f1-score']\n",
    "        autistic_f1_score = report['Autistic']['f1-score']\n",
    "\n",
    "        non_autistic_support = report['Non-Autistic']['support']\n",
    "        autistic_support = report['Autistic']['support']\n",
    "\n",
    "        '''# Aggregated metrics\n",
    "        accuracy = report['accuracy'''\n",
    "\n",
    "        macro_avg_precision = report['macro avg']['precision']\n",
    "        weighted_avg_precision = report['weighted avg']['precision']\n",
    "\n",
    "        macro_avg_recall = report['macro avg']['recall']\n",
    "        weighted_avg_recall = report['weighted avg']['recall']\n",
    "\n",
    "        macro_avg_f1_score = report['macro avg']['f1-score']\n",
    "        weighted_avg_f1_score = report['weighted avg']['f1-score']\n",
    "\n",
    "        macro_avg_support = report['macro avg']['support']\n",
    "        weighted_avg_support = report['weighted avg']['support']\n",
    "        \n",
    "        results[atlas_threshold][l_rate]['non_autistic_precision'] = non_autistic_precision\n",
    "        results[atlas_threshold][l_rate]['autistic_precision'] = autistic_precision\n",
    "        results[atlas_threshold][l_rate]['non_autistic_recall'] = non_autistic_recall\n",
    "        results[atlas_threshold][l_rate]['autistic_recall'] = autistic_recall\n",
    "        results[atlas_threshold][l_rate]['non_autistic_f1_score'] = non_autistic_f1_score\n",
    "        results[atlas_threshold][l_rate]['autistic_f1_score'] = autistic_f1_score\n",
    "        results[atlas_threshold][l_rate]['non_autistic_support'] = non_autistic_support\n",
    "        results[atlas_threshold][l_rate]['autistic_support'] = autistic_support\n",
    "        results[atlas_threshold][l_rate]['macro_avg_precision'] = macro_avg_precision\n",
    "        results[atlas_threshold][l_rate]['weighted_avg_precision'] = weighted_avg_precision\n",
    "        results[atlas_threshold][l_rate]['macro_avg_recall'] = macro_avg_recall\n",
    "        results[atlas_threshold][l_rate]['weighted_avg_recall'] = weighted_avg_recall\n",
    "        results[atlas_threshold][l_rate]['macro_avg_f1_score'] = macro_avg_f1_score\n",
    "        results[atlas_threshold][l_rate]['weighted_avg_f1_score'] = weighted_avg_f1_score\n",
    "        results[atlas_threshold][l_rate]['macro_avg_support'] = macro_avg_support\n",
    "        results[atlas_threshold][l_rate]['weighted_avg_support'] = weighted_avg_support\n",
    "        \n",
    "        atlas = None\n",
    "        l_rate = None\n",
    "        \n",
    "print(\"----Final Result----\")\n",
    "pprint.pprint(results)\n",
    "\n",
    "sorted_data = [(key, subkey, values['accuracy']) for key, subdata in results.items() for subkey, values in subdata.items()]\n",
    "sorted_data.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"----Sorted Accuracy----\")\n",
    "for key, subkey, accuracy in sorted_data:\n",
    "    print(f\"Key: {key}, Subkey: {subkey}, Accuracy: {accuracy}\")\n",
    "    \n",
    "# Create a list of tuples containing key, subkey, and corresponding details\n",
    "sorted_data = [(key, subkey, values) for key, subdata in results.items() for subkey, values in subdata.items()]\n",
    "\n",
    "# Sort the list based on the accuracy\n",
    "sorted_data.sort(key=lambda x: x[2]['accuracy'], reverse=True)\n",
    "\n",
    "# Create a new dictionary with the sorted order\n",
    "sorted_data_dict = {f\"{key}-{subkey}\": values for key, subkey, values in sorted_data}\n",
    "print(\"----Sorted Dict----\")\n",
    "pprint.pprint(sorted_data_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f1c4b29-566d-4e85-9d91-374b7f3f772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: cort-maxprob-thr25-2mm, Subkey: 0.01, Accuracy: 0.5\n",
      "Key: cort-maxprob-thr25-2mm, Subkey: 0.001, Accuracy: 0.5\n",
      "Key: cort-maxprob-thr25-2mm, Subkey: 0.0001, Accuracy: 0.5\n",
      "Key: sub-maxprob-thr0-1mm, Subkey: 0.01, Accuracy: 0.5\n",
      "Key: sub-maxprob-thr0-1mm, Subkey: 0.001, Accuracy: 0.5\n",
      "Key: sub-maxprob-thr0-1mm, Subkey: 0.0001, Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sorted_data = [(key, subkey, values['accuracy']) for key, subdata in results.items() for subkey, values in subdata.items()]\n",
    "sorted_data.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for key, subkey, accuracy in sorted_data:\n",
    "    print(f\"Key: {key}, Subkey: {subkey}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4dee641-2d1c-4a7e-ab23-b1c1fb213495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cort-maxprob-thr25-2mm-0.0001': {'accuracy': 0.5,\n",
      "                                   'autistic_f1_score': 0.0,\n",
      "                                   'autistic_precision': 0.0,\n",
      "                                   'autistic_recall': 0.0,\n",
      "                                   'autistic_support': 1,\n",
      "                                   'loss': 0.21887578070163727,\n",
      "                                   'macro_avg_f1_score': 0.3333333333333333,\n",
      "                                   'macro_avg_precision': 0.25,\n",
      "                                   'macro_avg_recall': 0.5,\n",
      "                                   'macro_avg_support': 2,\n",
      "                                   'non_autistic_f1_score': 0.6666666666666666,\n",
      "                                   'non_autistic_precision': 0.5,\n",
      "                                   'non_autistic_recall': 1.0,\n",
      "                                   'non_autistic_support': 1,\n",
      "                                   'weighted_avg_f1_score': 0.3333333333333333,\n",
      "                                   'weighted_avg_precision': 0.25,\n",
      "                                   'weighted_avg_recall': 0.5,\n",
      "                                   'weighted_avg_support': 2},\n",
      " 'cort-maxprob-thr25-2mm-0.001': {'accuracy': 0.5,\n",
      "                                  'autistic_f1_score': 0.0,\n",
      "                                  'autistic_precision': 0.0,\n",
      "                                  'autistic_recall': 0.0,\n",
      "                                  'autistic_support': 1,\n",
      "                                  'loss': 0.15739862620830536,\n",
      "                                  'macro_avg_f1_score': 0.3333333333333333,\n",
      "                                  'macro_avg_precision': 0.25,\n",
      "                                  'macro_avg_recall': 0.5,\n",
      "                                  'macro_avg_support': 2,\n",
      "                                  'non_autistic_f1_score': 0.6666666666666666,\n",
      "                                  'non_autistic_precision': 0.5,\n",
      "                                  'non_autistic_recall': 1.0,\n",
      "                                  'non_autistic_support': 1,\n",
      "                                  'weighted_avg_f1_score': 0.3333333333333333,\n",
      "                                  'weighted_avg_precision': 0.25,\n",
      "                                  'weighted_avg_recall': 0.5,\n",
      "                                  'weighted_avg_support': 2},\n",
      " 'cort-maxprob-thr25-2mm-0.01': {'accuracy': 0.5,\n",
      "                                 'autistic_f1_score': 0.0,\n",
      "                                 'autistic_precision': 0.0,\n",
      "                                 'autistic_recall': 0.0,\n",
      "                                 'autistic_support': 1,\n",
      "                                 'loss': 0.4831010103225708,\n",
      "                                 'macro_avg_f1_score': 0.3333333333333333,\n",
      "                                 'macro_avg_precision': 0.25,\n",
      "                                 'macro_avg_recall': 0.5,\n",
      "                                 'macro_avg_support': 2,\n",
      "                                 'non_autistic_f1_score': 0.6666666666666666,\n",
      "                                 'non_autistic_precision': 0.5,\n",
      "                                 'non_autistic_recall': 1.0,\n",
      "                                 'non_autistic_support': 1,\n",
      "                                 'weighted_avg_f1_score': 0.3333333333333333,\n",
      "                                 'weighted_avg_precision': 0.25,\n",
      "                                 'weighted_avg_recall': 0.5,\n",
      "                                 'weighted_avg_support': 2},\n",
      " 'sub-maxprob-thr0-1mm-0.0001': {'accuracy': 0.5,\n",
      "                                 'autistic_f1_score': 0.6666666666666666,\n",
      "                                 'autistic_precision': 0.5,\n",
      "                                 'autistic_recall': 1.0,\n",
      "                                 'autistic_support': 1,\n",
      "                                 'loss': 0.1447933316230774,\n",
      "                                 'macro_avg_f1_score': 0.3333333333333333,\n",
      "                                 'macro_avg_precision': 0.25,\n",
      "                                 'macro_avg_recall': 0.5,\n",
      "                                 'macro_avg_support': 2,\n",
      "                                 'non_autistic_f1_score': 0.0,\n",
      "                                 'non_autistic_precision': 0.0,\n",
      "                                 'non_autistic_recall': 0.0,\n",
      "                                 'non_autistic_support': 1,\n",
      "                                 'weighted_avg_f1_score': 0.3333333333333333,\n",
      "                                 'weighted_avg_precision': 0.25,\n",
      "                                 'weighted_avg_recall': 0.5,\n",
      "                                 'weighted_avg_support': 2},\n",
      " 'sub-maxprob-thr0-1mm-0.001': {'accuracy': 0.5,\n",
      "                                'autistic_f1_score': 0.6666666666666666,\n",
      "                                'autistic_precision': 0.5,\n",
      "                                'autistic_recall': 1.0,\n",
      "                                'autistic_support': 1,\n",
      "                                'loss': 0.21479275822639465,\n",
      "                                'macro_avg_f1_score': 0.3333333333333333,\n",
      "                                'macro_avg_precision': 0.25,\n",
      "                                'macro_avg_recall': 0.5,\n",
      "                                'macro_avg_support': 2,\n",
      "                                'non_autistic_f1_score': 0.0,\n",
      "                                'non_autistic_precision': 0.0,\n",
      "                                'non_autistic_recall': 0.0,\n",
      "                                'non_autistic_support': 1,\n",
      "                                'weighted_avg_f1_score': 0.3333333333333333,\n",
      "                                'weighted_avg_precision': 0.25,\n",
      "                                'weighted_avg_recall': 0.5,\n",
      "                                'weighted_avg_support': 2},\n",
      " 'sub-maxprob-thr0-1mm-0.01': {'accuracy': 0.5,\n",
      "                               'autistic_f1_score': 0.6666666666666666,\n",
      "                               'autistic_precision': 0.5,\n",
      "                               'autistic_recall': 1.0,\n",
      "                               'autistic_support': 1,\n",
      "                               'loss': 0.8909427523612976,\n",
      "                               'macro_avg_f1_score': 0.3333333333333333,\n",
      "                               'macro_avg_precision': 0.25,\n",
      "                               'macro_avg_recall': 0.5,\n",
      "                               'macro_avg_support': 2,\n",
      "                               'non_autistic_f1_score': 0.0,\n",
      "                               'non_autistic_precision': 0.0,\n",
      "                               'non_autistic_recall': 0.0,\n",
      "                               'non_autistic_support': 1,\n",
      "                               'weighted_avg_f1_score': 0.3333333333333333,\n",
      "                               'weighted_avg_precision': 0.25,\n",
      "                               'weighted_avg_recall': 0.5,\n",
      "                               'weighted_avg_support': 2}}\n",
      "Key: cort-maxprob-thr25-2mm-0.01\n",
      "Values: {'loss': 0.4831010103225708, 'accuracy': 0.5, 'non_autistic_precision': 0.5, 'autistic_precision': 0.0, 'non_autistic_recall': 1.0, 'autistic_recall': 0.0, 'non_autistic_f1_score': 0.6666666666666666, 'autistic_f1_score': 0.0, 'non_autistic_support': 1, 'autistic_support': 1, 'macro_avg_precision': 0.25, 'weighted_avg_precision': 0.25, 'macro_avg_recall': 0.5, 'weighted_avg_recall': 0.5, 'macro_avg_f1_score': 0.3333333333333333, 'weighted_avg_f1_score': 0.3333333333333333, 'macro_avg_support': 2, 'weighted_avg_support': 2}\n",
      "\n",
      "Key: cort-maxprob-thr25-2mm-0.001\n",
      "Values: {'loss': 0.15739862620830536, 'accuracy': 0.5, 'non_autistic_precision': 0.5, 'autistic_precision': 0.0, 'non_autistic_recall': 1.0, 'autistic_recall': 0.0, 'non_autistic_f1_score': 0.6666666666666666, 'autistic_f1_score': 0.0, 'non_autistic_support': 1, 'autistic_support': 1, 'macro_avg_precision': 0.25, 'weighted_avg_precision': 0.25, 'macro_avg_recall': 0.5, 'weighted_avg_recall': 0.5, 'macro_avg_f1_score': 0.3333333333333333, 'weighted_avg_f1_score': 0.3333333333333333, 'macro_avg_support': 2, 'weighted_avg_support': 2}\n",
      "\n",
      "Key: cort-maxprob-thr25-2mm-0.0001\n",
      "Values: {'loss': 0.21887578070163727, 'accuracy': 0.5, 'non_autistic_precision': 0.5, 'autistic_precision': 0.0, 'non_autistic_recall': 1.0, 'autistic_recall': 0.0, 'non_autistic_f1_score': 0.6666666666666666, 'autistic_f1_score': 0.0, 'non_autistic_support': 1, 'autistic_support': 1, 'macro_avg_precision': 0.25, 'weighted_avg_precision': 0.25, 'macro_avg_recall': 0.5, 'weighted_avg_recall': 0.5, 'macro_avg_f1_score': 0.3333333333333333, 'weighted_avg_f1_score': 0.3333333333333333, 'macro_avg_support': 2, 'weighted_avg_support': 2}\n",
      "\n",
      "Key: sub-maxprob-thr0-1mm-0.01\n",
      "Values: {'loss': 0.8909427523612976, 'accuracy': 0.5, 'non_autistic_precision': 0.0, 'autistic_precision': 0.5, 'non_autistic_recall': 0.0, 'autistic_recall': 1.0, 'non_autistic_f1_score': 0.0, 'autistic_f1_score': 0.6666666666666666, 'non_autistic_support': 1, 'autistic_support': 1, 'macro_avg_precision': 0.25, 'weighted_avg_precision': 0.25, 'macro_avg_recall': 0.5, 'weighted_avg_recall': 0.5, 'macro_avg_f1_score': 0.3333333333333333, 'weighted_avg_f1_score': 0.3333333333333333, 'macro_avg_support': 2, 'weighted_avg_support': 2}\n",
      "\n",
      "Key: sub-maxprob-thr0-1mm-0.001\n",
      "Values: {'loss': 0.21479275822639465, 'accuracy': 0.5, 'non_autistic_precision': 0.0, 'autistic_precision': 0.5, 'non_autistic_recall': 0.0, 'autistic_recall': 1.0, 'non_autistic_f1_score': 0.0, 'autistic_f1_score': 0.6666666666666666, 'non_autistic_support': 1, 'autistic_support': 1, 'macro_avg_precision': 0.25, 'weighted_avg_precision': 0.25, 'macro_avg_recall': 0.5, 'weighted_avg_recall': 0.5, 'macro_avg_f1_score': 0.3333333333333333, 'weighted_avg_f1_score': 0.3333333333333333, 'macro_avg_support': 2, 'weighted_avg_support': 2}\n",
      "\n",
      "Key: sub-maxprob-thr0-1mm-0.0001\n",
      "Values: {'loss': 0.1447933316230774, 'accuracy': 0.5, 'non_autistic_precision': 0.0, 'autistic_precision': 0.5, 'non_autistic_recall': 0.0, 'autistic_recall': 1.0, 'non_autistic_f1_score': 0.0, 'autistic_f1_score': 0.6666666666666666, 'non_autistic_support': 1, 'autistic_support': 1, 'macro_avg_precision': 0.25, 'weighted_avg_precision': 0.25, 'macro_avg_recall': 0.5, 'weighted_avg_recall': 0.5, 'macro_avg_f1_score': 0.3333333333333333, 'weighted_avg_f1_score': 0.3333333333333333, 'macro_avg_support': 2, 'weighted_avg_support': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a list of tuples containing key, subkey, and corresponding details\n",
    "sorted_data = [(key, subkey, values) for key, subdata in results.items() for subkey, values in subdata.items()]\n",
    "\n",
    "# Sort the list based on the accuracy\n",
    "sorted_data.sort(key=lambda x: x[2]['accuracy'], reverse=True)\n",
    "\n",
    "# Create a new dictionary with the sorted order\n",
    "sorted_data_dict = {f\"{key}-{subkey}\": values for key, subkey, values in sorted_data}\n",
    "pprint.pprint(sorted_data_dict)\n",
    "\n",
    "'''# Print the sorted dictionary\n",
    "for key, values in sorted_data_dict.items():\n",
    "    print(f\"Key: {key}\")\n",
    "    print(f\"Values: {values}\")\n",
    "    print()  # Adding a newline for better readability'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10dbec31-521b-4428-ad59-84456e46f2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    atlas learning_rate      loss  accuracy  \\\n",
      "0  cort-maxprob-thr25-2mm          0.01  0.306042       0.5   \n",
      "1  cort-maxprob-thr25-2mm         0.001  0.179666       0.5   \n",
      "2  cort-maxprob-thr25-2mm        0.0001  0.155944       1.0   \n",
      "\n",
      "   non_autistic_precision  autistic_precision  non_autistic_recall  \\\n",
      "0                     0.5                 0.0                  1.0   \n",
      "1                     0.5                 0.0                  1.0   \n",
      "2                     1.0                 1.0                  1.0   \n",
      "\n",
      "   autistic_recall  non_autistic_f1_score  autistic_f1_score  \\\n",
      "0              0.0               0.666667                0.0   \n",
      "1              0.0               0.666667                0.0   \n",
      "2              1.0               1.000000                1.0   \n",
      "\n",
      "   non_autistic_support  autistic_support  macro_avg_precision  \\\n",
      "0                     1                 1                 0.25   \n",
      "1                     1                 1                 0.25   \n",
      "2                     1                 1                 1.00   \n",
      "\n",
      "   weighted_avg_precision  macro_avg_recall  weighted_avg_recall  \\\n",
      "0                    0.25               0.5                  0.5   \n",
      "1                    0.25               0.5                  0.5   \n",
      "2                    1.00               1.0                  1.0   \n",
      "\n",
      "   macro_avg_f1_score  weighted_avg_f1_score  macro_avg_support  \\\n",
      "0            0.333333               0.333333                  2   \n",
      "1            0.333333               0.333333                  2   \n",
      "2            1.000000               1.000000                  2   \n",
      "\n",
      "   weighted_avg_support  \n",
      "0                     2  \n",
      "1                     2  \n",
      "2                     2  \n"
     ]
    }
   ],
   "source": [
    "# Convert the nested dictionary into a flat list of rows\n",
    "rows = []\n",
    "for atlas, atlas_data in results.items():\n",
    "    for lr, metrics in atlas_data.items():\n",
    "        row = {'atlas': atlas, 'learning_rate': lr}\n",
    "        row.update(metrics)\n",
    "        rows.append(row)\n",
    "\n",
    "# Create a DataFrame from the rows\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d54e337-c0c9-43b7-ab44-675a611b2552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Autistic': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1},\n",
      " 'Non-Autistic': {'f1-score': 1.0,\n",
      "                  'precision': 1.0,\n",
      "                  'recall': 1.0,\n",
      "                  'support': 1},\n",
      " 'accuracy': 1.0,\n",
      " 'macro avg': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 2},\n",
      " 'weighted avg': {'f1-score': 1.0,\n",
      "                  'precision': 1.0,\n",
      "                  'recall': 1.0,\n",
      "                  'support': 2}}\n",
      "Accuracy: 1.0\n",
      "Non-Autistic Precision: 1.0\n",
      "Autistic Precision: 1.0\n",
      "Non-Autistic Recall: 1.0\n",
      "Autistic Recall: 1.0\n",
      "Non-Autistic F1-Score: 1.0\n",
      "Autistic F1-Score: 1.0\n",
      "Non-Autistic Support: 1\n",
      "Autistic Support: 1\n",
      "Macro Average Precision: 1.0\n",
      "Weighted Average Precision: 1.0\n",
      "Macro Average Recall: 1.0\n",
      "Weighted Average Recall: 1.0\n",
      "Macro Average F1-Score: 1.0\n",
      "Weighted Average F1-Score: 1.0\n",
      "Macro Average Support: 2\n",
      "Weighted Average Support: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(all_labels, all_preds, target_names=['Non-Autistic', 'Autistic'], output_dict=True)\n",
    "\n",
    "pprint.pprint(report)\n",
    "\n",
    "# Individual class metrics\n",
    "# Access individual values\n",
    "non_autistic_precision = report['Non-Autistic']['precision']\n",
    "autistic_precision = report['Autistic']['precision']\n",
    "\n",
    "non_autistic_recall = report['Non-Autistic']['recall']\n",
    "autistic_recall = report['Autistic']['recall']\n",
    "\n",
    "non_autistic_f1_score = report['Non-Autistic']['f1-score']\n",
    "autistic_f1_score = report['Autistic']['f1-score']\n",
    "\n",
    "non_autistic_support = report['Non-Autistic']['support']\n",
    "autistic_support = report['Autistic']['support']\n",
    "\n",
    "# Aggregated metrics\n",
    "accuracy = report['accuracy']\n",
    "\n",
    "macro_avg_precision = report['macro avg']['precision']\n",
    "weighted_avg_precision = report['weighted avg']['precision']\n",
    "\n",
    "macro_avg_recall = report['macro avg']['recall']\n",
    "weighted_avg_recall = report['weighted avg']['recall']\n",
    "\n",
    "macro_avg_f1_score = report['macro avg']['f1-score']\n",
    "weighted_avg_f1_score = report['weighted avg']['f1-score']\n",
    "\n",
    "macro_avg_support = report['macro avg']['support']\n",
    "weighted_avg_support = report['weighted avg']['support']\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(\"Non-Autistic Precision:\", non_autistic_precision)\n",
    "print(\"Autistic Precision:\", autistic_precision)\n",
    "\n",
    "print(\"Non-Autistic Recall:\", non_autistic_recall)\n",
    "print(\"Autistic Recall:\", autistic_recall)\n",
    "\n",
    "print(\"Non-Autistic F1-Score:\", non_autistic_f1_score)\n",
    "print(\"Autistic F1-Score:\", autistic_f1_score)\n",
    "\n",
    "print(\"Non-Autistic Support:\", non_autistic_support)\n",
    "print(\"Autistic Support:\", autistic_support)\n",
    "\n",
    "print(\"Macro Average Precision:\", macro_avg_precision)\n",
    "print(\"Weighted Average Precision:\", weighted_avg_precision)\n",
    "\n",
    "print(\"Macro Average Recall:\", macro_avg_recall)\n",
    "print(\"Weighted Average Recall:\", weighted_avg_recall)\n",
    "\n",
    "print(\"Macro Average F1-Score:\", macro_avg_f1_score)\n",
    "print(\"Weighted Average F1-Score:\", weighted_avg_f1_score)\n",
    "\n",
    "print(\"Macro Average Support:\", macro_avg_support)\n",
    "print(\"Weighted Average Support:\", weighted_avg_support)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
