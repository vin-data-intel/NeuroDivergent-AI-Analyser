{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4722b-23c1-4ead-8783-05b86c49ebd2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import input_data, datasets\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = pd.read_csv(r\"/Users/vinoth/PycharmProjects/paper_implementation/Dataset/source/mri_images/ABIDE_pcp/Phenotypic_V1_0b_preprocessed1.csv\")\n",
    "\n",
    "# Replace labels 1 and 2 with 0 and 1\n",
    "csv_file['DX_GROUP'].replace({1: 0, 2: 1}, inplace=True)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_df, test_df = train_test_split(csv_file, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the Harvard-Oxford atlas\n",
    "atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "masker = input_data.NiftiLabelsMasker(labels_img=atlas.maps, standardize=True)\n",
    "\n",
    "# MRI file directory\n",
    "mri_dir = r\"/Users/vinoth/PycharmProjects/paper_implementation/Dataset/source/mri_images/ABIDE_pcp/cpac/nofilt_noglobal/\"\n",
    "\n",
    "# Placeholder for Graph Neural Network Data\n",
    "graph_data_list = []\n",
    "\n",
    "# Placeholder for time_series of all subjects\n",
    "time_series_all_subjects = []\n",
    "labels_all_subjects = []\n",
    "\n",
    "for idx, row in enumerate(train_df.itertuples()):\n",
    "    print(idx)\n",
    "    mri_filename = os.path.join(mri_dir, row.FILE_ID + \"_func_preproc.nii.gz\")\n",
    "    try:\n",
    "        mri_img = nib.load(mri_filename)\n",
    "\n",
    "        # Calculate time series\n",
    "        time_series = masker.fit_transform(mri_img)\n",
    "        print(time_series)\n",
    "        print(\"*******************************\")\n",
    "\n",
    "        # Store time series and corresponding label for all subjects\n",
    "        time_series_all_subjects.append(time_series)\n",
    "        labels_all_subjects.append(row.DX_GROUP)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "# Placeholder for adjacency matrices\n",
    "adjacency_matrices = []\n",
    "\n",
    "# Calculate the Granger causality for all pairs of ROIs\n",
    "count = 1\n",
    "for ts in time_series_all_subjects:\n",
    "    num_regions = ts.shape[1]\n",
    "    adjacency_matrix = np.zeros((num_regions, num_regions))\n",
    "\n",
    "    for i in range(num_regions):\n",
    "        for j in range(num_regions):\n",
    "            print(count)\n",
    "            if i != j:\n",
    "                # We adjust the maximum lag dynamically according to the length of time series data\n",
    "                maxlag = min(len(ts[:, i]), len(ts[:, j]), ts.shape[0]//3 - 1)\n",
    "                result = grangercausalitytests(ts[:, [i, j]], maxlag=maxlag, verbose=False)\n",
    "                p_values = [round(result[i+1][0]['ssr_ftest'][1], 4) for i in range(maxlag)]\n",
    "                # If the p-value is less than 0.05, then we say that region j G-causes region i\n",
    "                adjacency_matrix[i, j] = 1 if min(p_values) < 0.05 else 0\n",
    "                print(adjacency_matrix)\n",
    "                print(count)\n",
    "                print(\"###############################\")\n",
    "            count = count + 1\n",
    "\n",
    "    adjacency_matrices.append(adjacency_matrix)\n",
    "\n",
    "for idx, adjacency_matrix in enumerate(adjacency_matrices):\n",
    "\n",
    "    # Generate graph from adjacency matrix\n",
    "    G = nx.from_numpy_matrix(adjacency_matrix)\n",
    "\n",
    "    # Generate edges and features for PyTorch Geometric\n",
    "    edge_index = torch.tensor(list(G.edges), dtype=torch.long)\n",
    "    x = torch.tensor(time_series_all_subjects[idx], dtype=torch.float)\n",
    "    y = torch.tensor([labels_all_subjects[idx]], dtype=torch.float)\n",
    "\n",
    "    # Create graph data for PyTorch Geometric\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "\n",
    "    # Append data to list\n",
    "    graph_data_list.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9dff7d-8e2c-488d-b526-c2ea0bd01724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86544609-19a4-475d-bd84-8a86c7ddc870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db5875-e48c-4465-83da-67683538248e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6bf88-d66e-4533-b763-3fd62aa3bccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8066b1-763e-490f-8045-e37037dd61f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60712a91-2c17-4e8e-9fb6-5fd03d27d0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b25b7d-62d3-42d0-85bc-246ac3220079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b46d6b-6618-4f5e-94d1-bc8fe7d535fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import input_data, datasets\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def granger_test(params):\n",
    "    ts, i, j = params\n",
    "    maxlag = min(len(ts[:, i]), len(ts[:, j]), ts.shape[0]//3 - 1)\n",
    "    result = grangercausalitytests(ts[:, [i, j]], maxlag=maxlag, verbose=False)\n",
    "    p_values = [round(result[i+1][0]['ssr_ftest'][1], 4) for i in range(maxlag)]\n",
    "    return i, j, 1 if min(p_values) < 0.05 else 0\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = pd.read_csv(r\"/Users/vinoth/PycharmProjects/paper_implementation/Dataset/source/mri_images/ABIDE_pcp/Phenotypic_V1_0b_preprocessed1.csv\")\n",
    "\n",
    "# Replace labels 1 and 2 with 0 and 1\n",
    "csv_file['DX_GROUP'].replace({1: 0, 2: 1}, inplace=True)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_df, test_df = train_test_split(csv_file, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the Harvard-Oxford atlas\n",
    "atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "masker = input_data.NiftiLabelsMasker(labels_img=atlas.maps, standardize=True)\n",
    "\n",
    "# MRI file directory\n",
    "mri_dir = r\"/Users/vinoth/PycharmProjects/paper_implementation/Dataset/source/mri_images/ABIDE_pcp/cpac/nofilt_noglobal/\"\n",
    "\n",
    "# Placeholder for Graph Neural Network Data\n",
    "graph_data_list = []\n",
    "\n",
    "# Placeholder for time_series of all subjects\n",
    "time_series_all_subjects = []\n",
    "labels_all_subjects = []\n",
    "\n",
    "for idx, row in enumerate(train_df.itertuples()):\n",
    "    print(idx)\n",
    "    mri_filename = os.path.join(mri_dir, row.FILE_ID + \"_func_preproc.nii.gz\")\n",
    "    try:\n",
    "        mri_img = nib.load(mri_filename)\n",
    "\n",
    "        # Calculate time series\n",
    "        time_series = masker.fit_transform(mri_img)\n",
    "        print(time_series)\n",
    "        print(\"*******************************\")\n",
    "\n",
    "        # Store time series and corresponding label for all subjects\n",
    "        time_series_all_subjects.append(time_series)\n",
    "        labels_all_subjects.append(row.DX_GROUP)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "# Placeholder for adjacency matrices\n",
    "count = 1\n",
    "for ts in time_series_all_subjects:\n",
    "    num_regions = ts.shape[1]\n",
    "    adjacency_matrix = np.zeros((num_regions, num_regions))\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        params = [(ts, i, j) for i in range(num_regions) for j in range(num_regions) if i != j]\n",
    "        results = executor.map(granger_test, params)\n",
    "\n",
    "        for i, j, result in results:\n",
    "            adjacency_matrix[i, j] = result\n",
    "            count += 1\n",
    "\n",
    "    adjacency_matrices.append(adjacency_matrix)\n",
    "\n",
    "for idx, adjacency_matrix in enumerate(adjacency_matrices):\n",
    "\n",
    "    # Generate graph from adjacency matrix\n",
    "    G = nx.from_numpy_matrix(adjacency_matrix)\n",
    "\n",
    "    # Generate edges and features for PyTorch Geometric\n",
    "    edge_index = torch.tensor(list(G.edges), dtype=torch.long)\n",
    "    x = torch.tensor(time_series_all_subjects[idx], dtype=torch.float)\n",
    "    y = torch.tensor([labels_all_subjects[idx]], dtype=torch.float)\n",
    "\n",
    "    # Create graph data for PyTorch Geometric\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "\n",
    "    # Append data to list\n",
    "    graph_data_list.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ab0b3-ff63-46c3-ad94-9948beebbb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf9e32-ffe1-4082-b4c0-ae1622cc2712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3160041-272a-4eb4-b175-854cbff581ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4962a73-2eae-4919-b24f-4e78d01a3d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683dae70-86e9-4e94-9fe5-821aea748b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578af42-91f5-4b0e-8f82-87e1d636bcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b9909-3aa8-49ca-a202-443de24bde10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ecf89b-1120-442b-96ad-221fa47fb150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42c6b9-3f6c-4bc7-9737-16a27a287449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv, global_mean_pool\n",
    "from torch_geometric.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, conv_layer, num_node_features, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        if conv_layer == GINConv:\n",
    "            nn1 = torch.nn.Sequential(torch.nn.Linear(num_node_features, 16), torch.nn.ReLU(), torch.nn.Linear(16, 16))\n",
    "            self.conv1 = conv_layer(nn1)\n",
    "            nn2 = torch.nn.Sequential(torch.nn.Linear(16, 32), torch.nn.ReLU(), torch.nn.Linear(32, 32))\n",
    "            self.conv2 = conv_layer(nn2)\n",
    "        else:\n",
    "            self.conv1 = conv_layer(num_node_features, 16)\n",
    "            self.conv2 = conv_layer(16, 32)\n",
    "        self.fc = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        x = global_mean_pool(x, batch)  # Pooling\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "# Define models and their names\n",
    "models = [GCNConv, GATConv, SAGEConv, GINConv]\n",
    "model_names = ['GCN', 'GAT', 'SAGE', 'GIN']\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "results = pd.DataFrame(columns=[\"Model\", \"Train_Accuracy\", \"Test_Accuracy\"])\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = pd.DataFrame(columns=[\"Model\", \"Train_Accuracy\", \"Test_Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "\n",
    "# Model Training and Evaluation for each Convolution Layer\n",
    "for model_name, model_class in zip(model_names, models):\n",
    "    model = Net(model_class, num_features, num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = F.nll_loss(out, data.y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'{model_name} - Epoch: {epoch+1}, Loss: {total_loss/len(loader)}')\n",
    "\n",
    "    # Evaluation on Training Data\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        _, pred = model(data).max(dim=1)\n",
    "        correct += int((pred == data.y.long()).sum())\n",
    "    train_accuracy = correct / len(loader.dataset)\n",
    "    print(f'{model_name} Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Evaluation on Test Data\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            _, pred = output.max(dim=1)\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        all_labels.append(data.y.cpu().numpy())\n",
    "        correct += int((pred == data.y.long()).sum())\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    print(f'{model_name} Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    # Flatten the list of predictions and labels\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt=\".2%\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix (Normalized) for {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=['Non-Autistic', 'Autistic'], output_dict=True)\n",
    "\n",
    "    # Append the accuracy to the results DataFrame\n",
    "    results = results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Train_Accuracy\": train_accuracy,\n",
    "        \"Test_Accuracy\": test_accuracy,\n",
    "        \"Precision\": report['macro avg']['precision'],\n",
    "        \"Recall\": report['macro avg']['recall'],\n",
    "        \"F1-Score\": report['macro avg']['f1-score'],\n",
    "    }, ignore_index=True)\n",
    "\n",
    "print(results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
