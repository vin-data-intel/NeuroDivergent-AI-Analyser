{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da20452b-70bb-4e50-a51a-455ab3651155",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Threshold----\n",
      "cort-maxprob-thr25-2mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/889 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 236\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/889 [10:35<156:40:34, 635.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 116\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 2/889 [13:19<88:18:25, 358.41s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 196\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 3/889 [21:05<100:15:38, 407.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 176\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 4/889 [26:40<93:06:01, 378.71s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 116\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 5/889 [29:09<72:39:23, 295.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 296\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 6/889 [44:40<125:33:28, 511.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 296\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 8/889 [1:02:15<127:10:49, 519.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 196\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 9/889 [1:09:11<120:23:25, 492.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 196\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 10/889 [1:15:56<114:27:56, 468.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 152\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 11/889 [1:19:59<98:59:59, 405.92s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 296\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▏         | 12/889 [1:35:33<135:31:04, 556.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 236\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▏         | 13/889 [1:45:41<139:03:49, 571.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 176\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 14/889 [1:51:21<122:24:38, 503.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 152\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 15/889 [1:55:40<104:45:27, 431.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 246\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 16/889 [2:06:43<121:15:33, 500.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 116\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 17/889 [2:09:13<95:56:15, 396.07s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 296\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 18/889 [2:25:11<136:19:35, 563.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 246\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 20/889 [2:36:18<110:29:04, 457.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 176\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 22/889 [2:41:48<82:17:40, 341.71s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 176\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 24/889 [2:47:15<66:35:42, 277.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 116\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 25/889 [2:49:35<59:46:28, 249.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 196\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 26/889 [2:56:23<68:19:38, 285.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 296\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 27/889 [3:11:52<106:01:36, 442.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 116\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 28/889 [3:14:14<87:16:41, 364.93s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 236\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 30/889 [3:24:06<80:04:32, 335.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 176\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▎         | 33/889 [3:29:34<54:08:00, 227.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 296\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 34/889 [3:44:51<84:22:10, 355.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 116\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 36/889 [3:47:11<60:48:20, 256.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 196\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 37/889 [3:53:51<67:27:03, 285.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 78\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 38/889 [3:54:55<55:50:28, 236.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 196\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 39/889 [4:01:37<65:09:14, 275.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 176\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 40/889 [4:07:01<67:55:44, 288.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 296\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▍         | 41/889 [4:22:14<107:10:12, 454.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 78\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▍         | 42/889 [4:23:18<81:39:19, 347.06s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 146\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▍         | 44/889 [4:27:03<57:12:03, 243.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 206\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 45/889 [4:34:32<68:39:05, 292.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 116\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 47/889 [4:36:55<47:26:15, 202.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 196\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 48/889 [4:43:40<58:00:54, 248.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 236\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 49/889 [4:53:27<77:10:47, 330.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 176\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 50/889 [4:58:54<76:50:18, 329.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 196\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 51/889 [5:05:38<81:26:36, 349.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 236\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 52/889 [5:15:25<96:37:26, 415.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 116\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 53/889 [5:17:46<78:24:56, 337.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 176\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 54/889 [5:23:14<77:38:23, 334.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 246\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 55/889 [5:33:51<98:02:27, 423.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 146\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▋         | 56/889 [5:37:36<84:24:54, 364.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 246\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▋         | 57/889 [5:48:15<103:04:00, 445.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 236\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 60/889 [5:58:01<70:25:12, 305.81s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 206\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 61/889 [6:05:29<77:12:37, 335.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 196\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 62/889 [6:12:13<80:44:19, 351.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 116\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 64/889 [6:14:35<54:59:11, 239.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 236\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 65/889 [6:24:22<72:29:54, 316.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 176\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 66/889 [6:29:48<72:56:06, 319.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 296\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 67/889 [6:45:12<108:05:10, 473.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 296\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 68/889 [7:00:38<135:32:24, 594.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 236\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 69/889 [7:10:25<134:54:27, 592.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 176\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 70/889 [7:15:51<117:37:07, 517.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 146\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 71/889 [7:19:35<98:20:15, 432.78s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 246\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 72/889 [7:30:15<111:53:19, 493.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 116\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 73/889 [7:32:37<88:21:33, 389.82s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 78\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 74/889 [7:33:41<66:27:53, 293.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 236\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 75/889 [7:43:29<86:08:12, 380.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 152\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|▊         | 76/889 [7:47:34<76:53:15, 340.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 176\n",
      "Number of time points: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|▊         | 77/889 [7:53:01<75:53:02, 336.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 246\n",
      "Number of time points: 48\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import input_data, datasets\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from fastdtw import fastdtw\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.nn import BatchNorm1d, Dropout\n",
    "import os\n",
    "from nilearn import plotting\n",
    "from nilearn import image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pprint\n",
    "# Ignore all warnings (not recommended unless you know what you are doing)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.api import VAR\n",
    "            \n",
    "\n",
    "\n",
    "'''train_data_eda = \"cort-maxprob-thr25-2mm_0_8_GRC_train_eda/\"\n",
    "os.makedirs(train_data_eda, exist_ok=True)'''\n",
    "\n",
    "test_result_dir = \"cort-maxprob-thr25-2mm_0_8_GRC_test_result/\"\n",
    "os.makedirs(test_result_dir, exist_ok=True)\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = pd.read_csv(r\"/Users/vinoth/PycharmProjects/paper_implementation/Dataset/source/mri_images/ABIDE_pcp/Phenotypic_V1_0b_preprocessed1.csv\")\n",
    "csv_file['DX_GROUP'].replace({1: 0, 2: 1}, inplace=True)\n",
    "train_df, test_df = train_test_split(csv_file, test_size=0.2, random_state=42)\n",
    "harvard_oxford_atlas = ['cort-maxprob-thr25-2mm']\n",
    "'''values = [\n",
    "    \"cort-maxprob-thr0-1mm\",\n",
    "    \"cort-maxprob-thr0-2mm\",\n",
    "    \"cort-maxprob-thr25-1mm\",\n",
    "    \"cort-maxprob-thr25-2mm\",\n",
    "    \"cort-maxprob-thr50-1mm\",\n",
    "    \"cort-maxprob-thr50-2mm\",\n",
    "    \"cort-prob-1mm\",\n",
    "    \"cort-prob-2mm\",\n",
    "    \"cortl-maxprob-thr0-1mm\",\n",
    "    \"cortl-maxprob-thr0-2mm\",\n",
    "    \"cortl-maxprob-thr25-1mm\",\n",
    "    \"cortl-maxprob-thr25-2mm\",\n",
    "    \"cortl-maxprob-thr50-1mm\",\n",
    "    \"cortl-maxprob-thr50-2mm\",\n",
    "    \"cortl-prob-1mm\",\n",
    "    \"cortl-prob-2mm\",\n",
    "    \"sub-maxprob-thr0-1mm\",\n",
    "    \"sub-maxprob-thr0-2mm\",\n",
    "    \"sub-maxprob-thr25-1mm\",\n",
    "    \"sub-maxprob-thr25-2mm\",\n",
    "    \"sub-maxprob-thr50-1mm\",\n",
    "    \"sub-maxprob-thr50-2mm\",\n",
    "    \"sub-prob-1mm\",\n",
    "    \"sub-prob-2mm\"\n",
    "]'''\n",
    "\n",
    "\n",
    "results = {}\n",
    "atlas_threshold = None\n",
    "\n",
    "for data in tqdm(harvard_oxford_atlas):\n",
    "    atlas_threshold = data\n",
    "    print(\"----Threshold----\")\n",
    "    print(atlas_threshold)\n",
    "    results[atlas_threshold] = {}\n",
    "    atlas = datasets.fetch_atlas_harvard_oxford(data)\n",
    "    masker = input_data.NiftiLabelsMasker(labels_img=atlas.maps, standardize=True)\n",
    "    mri_dir = r\"/Users/vinoth/PycharmProjects/paper_implementation/Dataset/source/mri_images/ABIDE_pcp/cpac/nofilt_noglobal/\"\n",
    "\n",
    "    # Placeholder for Graph Neural Network Data\n",
    "    graph_data_list = []\n",
    "\n",
    "    # Data Preprocessing\n",
    "    for idx, row in tqdm(enumerate(train_df.itertuples()), total=len(train_df)):\n",
    "        # Combine the parent and nested folder paths\n",
    "        '''file_dir = os.path.join(train_data_eda, row.FILE_ID)\n",
    "        os.makedirs(file_dir, exist_ok=True)'''\n",
    "        mri_filename = os.path.join(mri_dir, row.FILE_ID + \"_func_preproc.nii.gz\")\n",
    "        \n",
    "        try:\n",
    "            mri_img = nib.load(mri_filename)\n",
    "            \n",
    "            '''mri_img_dir = os.path.join(file_dir, 'mri_image')\n",
    "            os.makedirs(mri_img_dir, exist_ok=True)\n",
    "            \n",
    "            # Select the first time point\n",
    "            first_volume = mri_img.slicer[:,:,:,0]\n",
    "            \n",
    "            image_shape = mri_img.shape\n",
    "\n",
    "            # The total number of volumes in the 4D dimension is the size of the fourth dimension\n",
    "            total_volumes = image_shape[3]\n",
    "\n",
    "            #print(\"Total number of volumes in the 4D image for file \" + row.FILE_ID + \" : \", total_volumes)'''\n",
    "\n",
    "            '''# Plot the image\n",
    "            plotting.plot_img(first_volume, cmap='gray')  # grayscale often works well for MRIs\n",
    "            filename = os.path.join(mri_img_dir, row.FILE_ID+'_img.png')\n",
    "            plt.savefig(filename)\n",
    "            plt.close()  # Close the plot to avoid overlaps\n",
    "\n",
    "            # Plot the EPI\n",
    "            plotting.plot_epi(first_volume, display_mode='z', cut_coords=5, cmap='viridis')  # viridis is a perceptually uniform colormap\n",
    "            filename = os.path.join(mri_img_dir, row.FILE_ID+'_epi_img.png')\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "\n",
    "            # Plot the anatomy\n",
    "            plotting.plot_anat(first_volume, cmap='gray')  # grayscale again for anatomical images\n",
    "            filename = os.path.join(mri_img_dir, row.FILE_ID+'_anat_img.png')\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "\n",
    "            # Plot the statistical map\n",
    "            plotting.plot_stat_map(first_volume, bg_img=None, threshold=3.0, cmap='cold_hot')  # cold_hot is often used for stat maps\n",
    "            filename = os.path.join(mri_img_dir, row.FILE_ID+'_stat_map_img.png')\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "\n",
    "            # Plot the probabilistic atlas\n",
    "            plotting.plot_prob_atlas(mri_img, bg_img=None, colorbar=True)  # default colormap should work for probabilistic atlas\n",
    "            filename = os.path.join(mri_img_dir, row.FILE_ID+'_atlas_map_img.png')\n",
    "            plt.savefig(filename)\n",
    "            plt.close()'''\n",
    "            \n",
    "            # Assuming masker and mri_img are already defined\n",
    "            time_series = masker.fit_transform(mri_img)\n",
    "\n",
    "            n_regions, n_time_points = time_series.shape\n",
    "            number_of_entries = n_regions * n_time_points\n",
    "            \n",
    "            '''n_time_points, n_regions = time_series.shape\n",
    "            assert n_regions == 48, f\"Expected 48 regions but got {n_regions}\"'''\n",
    "\n",
    "            \n",
    "            \n",
    "            # Assuming masker and mri_img are already defined\n",
    "\n",
    "            n_regions, n_time_points = time_series.shape\n",
    "            print(f\"Number of regions: {n_regions}\")\n",
    "            print(f\"Number of time points: {n_time_points}\")\n",
    "\n",
    "            # Adjust the maxlag based on the number of time points you have\n",
    "            maxlag = min(12, n_time_points - 1) \n",
    "\n",
    "\n",
    "            # Create an adjacency matrix based on Granger causality\n",
    "            granger_matrix = np.zeros((n_regions, n_regions))\n",
    "\n",
    "            for i in range(n_regions):\n",
    "                for j in range(n_regions):\n",
    "                    if i != j:\n",
    "                        data_for_test = time_series[[i, j], :].T\n",
    "\n",
    "                        model = VAR(data_for_test)\n",
    "\n",
    "                        # Set a range of lags for estimation\n",
    "                        max_lags_range = list(range(1, 13))  # This sets lags from 1 to 12. Adjust as necessary.\n",
    "\n",
    "                        best_aic = np.inf\n",
    "                        best_lag = None\n",
    "\n",
    "                        for lag in max_lags_range:\n",
    "                            try:\n",
    "                                result = model.fit(maxlags=lag)\n",
    "                                if result.aic < best_aic:\n",
    "                                    best_aic = result.aic\n",
    "                                    best_lag = lag\n",
    "                            except Exception:\n",
    "                                pass  # Skip this lag if there's an error\n",
    "\n",
    "                        # If a best lag was found, use it to fit the model and test causality\n",
    "                        if best_lag:\n",
    "                            result = model.fit(maxlags=best_lag)\n",
    "                            test_result = result.test_causality(0, 1, kind='f', signif=0.05)\n",
    "                            p_value = test_result.pvalue\n",
    "                            granger_matrix[i, j] = p_value\n",
    "                        else:\n",
    "                            # Handle cases where no lag could be estimated. For now, we'll just print a message.\n",
    "                            print(f\"Could not estimate VAR model for regions {i} and {j}.\")\n",
    "\n",
    "\n",
    "\n",
    "            # Generate graph from Granger causality matrix\n",
    "            G = nx.from_numpy_matrix(granger_matrix)\n",
    "            \n",
    "\n",
    "            '''if idx == 0:  # Only for the first iteration\n",
    "                # Plot the time series for the regions\n",
    "                plt.figure(figsize=(35, 15))\n",
    "                for i in range(min(n_regions, time_series.shape[0])):\n",
    "                    plt.plot(time_series[i, :], label=f'Region {i + 1}')\n",
    "                plt.xlabel('Time point')\n",
    "                plt.ylabel('Blood Oxygen Level(BOLD) - Normalized signal')\n",
    "                plt.title('Time series of the regions')\n",
    "                plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "                plt.tight_layout()\n",
    "            \n",
    "                # Save the plot to the existing folder\n",
    "                time_series_filename = row.FILE_ID+'_'+'time_series_plot.png'\n",
    "                \n",
    "                plt.savefig(os.path.join(file_dir, time_series_filename))\n",
    "                \n",
    "                plt.figure(figsize=(10, 10))\n",
    "                sns.heatmap(similarity_matrix, annot=False, cmap='turbo')\n",
    "                plt.title('Similarity Matrix')\n",
    "                similarity_matrix_adj_img_filename = row.FILE_ID + '_similarity_matrix.png'\n",
    "                plt.savefig(os.path.join(file_dir, similarity_matrix_adj_img_filename))\n",
    "                plt.close() # Close the plot\n",
    "                similarity_matrix_npy_filename = row.FILE_ID + '_similarity_matrix.npy'\n",
    "                similarity_matrix_npy_path = os.path.join(file_dir, similarity_matrix_npy_filename)\n",
    "                np.save(similarity_matrix_npy_path, similarity_matrix)\n",
    "\n",
    "                # Visualize the graph\n",
    "                plt.figure(figsize=(45, 25))\n",
    "                pos = nx.spring_layout(G)\n",
    "\n",
    "                # Extract the edge weights from the graph\n",
    "                weights = [G[u][v].get('weight', 1) for u, v in G.edges()]\n",
    "\n",
    "                # Normalize the weights to fit your desired range of thickness\n",
    "                normalized_weights = [5 * weight / max(weights) for weight in weights]\n",
    "\n",
    "                # Draw the edges with the thickness determined by the normalized weights\n",
    "                nx.draw_networkx_edges(G, pos, width=normalized_weights)\n",
    "\n",
    "                # Draw the nodes and labels\n",
    "                nx.draw_networkx_nodes(G, pos)\n",
    "                nx.draw_networkx_labels(G, pos)\n",
    "                \n",
    "                # Define the path and filename where you want to save the plot\n",
    "                graph_plot_filename = row.FILE_ID + '_graph_plot.png'\n",
    "                graph_plot_path = os.path.join(file_dir, graph_plot_filename)\n",
    "\n",
    "                # Save the plot to the specified path\n",
    "                plt.savefig(graph_plot_path'''\n",
    "\n",
    "            edge_index = torch.tensor(list(G.edges), dtype=torch.long)\n",
    "            x = torch.tensor(time_series, dtype=torch.float)\n",
    "            y = torch.tensor([row.DX_GROUP], dtype=torch.float)\n",
    "            data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "            graph_data_list.append(data)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    '''# Neural Network Model with Regularization, Batch Normalization, and Dropout\n",
    "    class Net(torch.nn.Module):\n",
    "        def __init__(self, num_node_features, num_classes):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = GCNConv(num_node_features, 16)\n",
    "            self.bn1 = BatchNorm1d(16)\n",
    "            self.conv2 = GCNConv(16, 32)\n",
    "            self.bn2 = BatchNorm1d(32)\n",
    "            self.fc = torch.nn.Linear(32, num_classes)\n",
    "            self.dropout = Dropout(0.5)\n",
    "\n",
    "        def forward(self, data):\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = self.bn2(x)\n",
    "            x = global_mean_pool(x, batch)\n",
    "            x = self.fc(x)\n",
    "            return F.log_softmax(x, dim=1)'''\n",
    "\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch.nn import BatchNorm1d, Dropout, Linear\n",
    "    from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "    class Net(torch.nn.Module):\n",
    "        def __init__(self, num_node_features, num_classes):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = GATConv(num_node_features, 16, heads=1, concat=True)\n",
    "            self.bn1 = BatchNorm1d(16)\n",
    "            self.conv2 = GATConv(16, 32, heads=1, concat=True)\n",
    "            self.bn2 = BatchNorm1d(32)\n",
    "            self.fc = Linear(32, num_classes)\n",
    "            self.dropout = Dropout(0.5)\n",
    "\n",
    "        def forward(self, data):\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = self.bn2(x)\n",
    "            x = global_mean_pool(x, batch)\n",
    "            x = self.fc(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_features = graph_data_list[0].num_node_features\n",
    "    num_classes = 2\n",
    "    model = Net(num_features, num_classes).to(device)\n",
    "    loader = DataLoader(graph_data_list, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Hyperparameter Tuning (Example: Adjusting Learning Rate)\n",
    "    learning_rates = [0.01, 0.001, 0.0001]\n",
    "    l_rate = None\n",
    "    train_losses = {}\n",
    "    test_accuracies = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        l_rate = str(lr)\n",
    "        train_losses[l_rate] = []\n",
    "        test_accuracies[l_rate] = []\n",
    "        results[atlas_threshold][l_rate] = {}\n",
    "        print(\"Learning Rate --> \", lr)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4) # L2 Regularization\n",
    "        for epoch in tqdm(range(70)):\n",
    "            total_loss = 0\n",
    "            model.train()\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data)\n",
    "                loss = F.nll_loss(out, data.y.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            train_losses[l_rate].append(total_loss/len(loader))\n",
    "            print(f'Epoch: {epoch+1}, Loss: {total_loss/len(loader)}')\n",
    "        results[atlas_threshold][l_rate]['loss'] = total_loss/len(loader)\n",
    "\n",
    "        # Placeholder for time series data\n",
    "        time_series_list = []\n",
    "        successful_indices = []\n",
    "\n",
    "        # Testing Data Preprocessing\n",
    "        # for idx, row in enumerate(test_df.itertuples()):\n",
    "        for idx, row in tqdm(enumerate(test_df.itertuples()), total=len(test_df)):\n",
    "            '''if idx == 2:\n",
    "                break'''\n",
    "            mri_filename = os.path.join(mri_dir, row.FILE_ID + \"_func_preproc.nii.gz\")\n",
    "            try:\n",
    "                mri_img = nib.load(mri_filename)\n",
    "                time_series = masker.fit_transform(mri_img)\n",
    "                time_series_list.append(time_series)\n",
    "                successful_indices.append(idx)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "        \n",
    "        # Placeholder for Graph Neural Network Data for testing\n",
    "        graph_data_test_list = []\n",
    "        \n",
    "        \n",
    "        from tqdm import tqdm\n",
    "        from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "        def compute_granger_matrix(time_series, maxlags_range):\n",
    "            n_regions = time_series.shape[1]\n",
    "            result_matrix = np.zeros((n_regions, n_regions))\n",
    "\n",
    "            for i in range(n_regions):\n",
    "                for j in range(n_regions):\n",
    "                    if i != j:\n",
    "                        # Safety check to catch out-of-bounds indices\n",
    "                        assert i < n_regions, f\"Index i={i} is out of bounds!\"\n",
    "                        assert j < n_regions, f\"Index j={j} is out of bounds!\"\n",
    "\n",
    "                        best_p_value = 1.0  # Initialize with the highest possible p-value\n",
    "\n",
    "                        for lag in maxlags_range:\n",
    "                            try:\n",
    "                                test_result = grangercausalitytests(time_series[:, [i, j]], maxlag=lag, verbose=False)\n",
    "                                p_value = min([test_result[k+1][0]['ssr_ftest'][1] for k in range(lag)])\n",
    "                                if p_value < best_p_value:\n",
    "                                    best_p_value = p_value\n",
    "                            except Exception:\n",
    "                                pass  # Skip this lag if there's an error\n",
    "\n",
    "                        result_matrix[i, j] = best_p_value\n",
    "\n",
    "            return result_matrix\n",
    "\n",
    "        # Assuming successful_indices and time_series_list are already defined\n",
    "        for idx, successful_idx in tqdm(enumerate(successful_indices), total=len(successful_indices)):\n",
    "            #row = test_df.iloc[successful_idx]  # Uncomment if you need this row\n",
    "            time_series = time_series_list[idx]\n",
    "\n",
    "            # Define a range of lags for Granger causality test\n",
    "            maxlags_range = list(range(1, 13))  # Adjust as necessary\n",
    "\n",
    "            granger_matrix = compute_granger_matrix(time_series.T, maxlags_range)\n",
    "\n",
    "            # Threshold the Granger causality matrix (optional)\n",
    "            threshold = 0.05  # Set a suitable threshold for p-values\n",
    "            granger_matrix[granger_matrix > threshold] = 0\n",
    "\n",
    "            # Generate graph from Granger causality matrix\n",
    "            G = nx.from_numpy_matrix(granger_matrix, create_using=nx.DiGraph)\n",
    "            edge_index = torch.tensor(list(G.edges), dtype=torch.long)\n",
    "            x = torch.tensor(time_series, dtype=torch.float)\n",
    "            y = torch.tensor([row.DX_GROUP], dtype=torch.float)\n",
    "            data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "            graph_data_test_list.append(data)\n",
    "\n",
    "\n",
    "        # Create a data loader for testing data\n",
    "        test_loader = DataLoader(graph_data_test_list, batch_size=32, shuffle=False)\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for data in tqdm(test_loader):\n",
    "            data = data.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "                _, pred = output.max(dim=1)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_labels.append(data.y.cpu().numpy())\n",
    "            correct += int((pred == data.y.long()).sum())\n",
    "\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        test_accuracies[l_rate].append(accuracy)\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "        results[atlas_threshold][l_rate]['accuracy'] = accuracy\n",
    "        \n",
    "        # Flatten the list of predictions and labels\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        \n",
    "        # Specify the parent folder\n",
    "        parent_folder = test_result_dir\n",
    "\n",
    "        # Specify the nested folder names\n",
    "        nested_folder1 = atlas_threshold\n",
    "        nested_folder2 = 'learning_rate_'+l_rate\n",
    "        \n",
    "        # Combine the parent and nested folder pa'hs\n",
    "        validation_result_dir = os.path.join(parent_folder, nested_folder1, nested_folder2)\n",
    "        \n",
    "        # Create the nested folders, including any necessary parent directories\n",
    "        os.makedirs(validation_result_dir, exist_ok=True)\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(cm, annot=True, cmap='Blues', fmt=\".2%\")\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix (Normalized)')\n",
    "        \n",
    "        # Save the image inside the nested folder\n",
    "        plt.savefig(os.path.join(validation_result_dir, 'confusion_matrix.png'))\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        # Print actual vs predicted\n",
    "        actual_vs_predicted = pd.DataFrame({'Actual': all_labels, 'Predicted': all_preds})\n",
    "        print(actual_vs_predicted)\n",
    "\n",
    "        # Classification report\n",
    "        report = classification_report(all_labels, all_preds, target_names=['Non-Autistic', 'Autistic'], output_dict=True)\n",
    "        #print(classification_report(all_labels, all_preds, target_names=['Non-Autistic', 'Autistic']))\n",
    "        report_text = classification_report(all_labels, all_preds, target_names=['Non-Autistic', 'Autistic'])\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.text(0.01, 0.05, report_text, {'fontsize': 12}, fontproperties='monospace') # Adjust text size and position accordingly\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(validation_result_dir, 'classification_report.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        with open(os.path.join(validation_result_dir, 'classification_report.txt'), 'w') as file:\n",
    "            file.write(report_text)\n",
    "  \n",
    "        # Individual class metrics\n",
    "        # Access individual values\n",
    "        non_autistic_precision = report['Non-Autistic']['precision']\n",
    "        autistic_precision = report['Autistic']['precision']\n",
    "\n",
    "        non_autistic_recall = report['Non-Autistic']['recall']\n",
    "        autistic_recall = report['Autistic']['recall']\n",
    "\n",
    "        non_autistic_f1_score = report['Non-Autistic']['f1-score']\n",
    "        autistic_f1_score = report['Autistic']['f1-score']\n",
    "\n",
    "        non_autistic_support = report['Non-Autistic']['support']\n",
    "        autistic_support = report['Autistic']['support']\n",
    "\n",
    "        '''# Aggregated metrics\n",
    "        accuracy = report['accuracy'''\n",
    "\n",
    "        macro_avg_precision = report['macro avg']['precision']\n",
    "        weighted_avg_precision = report['weighted avg']['precision']\n",
    "\n",
    "        macro_avg_recall = report['macro avg']['recall']\n",
    "        weighted_avg_recall = report['weighted avg']['recall']\n",
    "\n",
    "        macro_avg_f1_score = report['macro avg']['f1-score']\n",
    "        weighted_avg_f1_score = report['weighted avg']['f1-score']\n",
    "\n",
    "        macro_avg_support = report['macro avg']['support']\n",
    "        weighted_avg_support = report['weighted avg']['support']\n",
    "        \n",
    "        results[atlas_threshold][l_rate]['non_autistic_precision'] = non_autistic_precision\n",
    "        results[atlas_threshold][l_rate]['autistic_precision'] = autistic_precision\n",
    "        results[atlas_threshold][l_rate]['non_autistic_recall'] = non_autistic_recall\n",
    "        results[atlas_threshold][l_rate]['autistic_recall'] = autistic_recall\n",
    "        results[atlas_threshold][l_rate]['non_autistic_f1_score'] = non_autistic_f1_score\n",
    "        results[atlas_threshold][l_rate]['autistic_f1_score'] = autistic_f1_score\n",
    "        results[atlas_threshold][l_rate]['non_autistic_support'] = non_autistic_support\n",
    "        results[atlas_threshold][l_rate]['autistic_support'] = autistic_support\n",
    "        results[atlas_threshold][l_rate]['macro_avg_precision'] = macro_avg_precision\n",
    "        results[atlas_threshold][l_rate]['weighted_avg_precision'] = weighted_avg_precision\n",
    "        results[atlas_threshold][l_rate]['macro_avg_recall'] = macro_avg_recall\n",
    "        results[atlas_threshold][l_rate]['weighted_avg_recall'] = weighted_avg_recall\n",
    "        results[atlas_threshold][l_rate]['macro_avg_f1_score'] = macro_avg_f1_score\n",
    "        results[atlas_threshold][l_rate]['weighted_avg_f1_score'] = weighted_avg_f1_score\n",
    "        results[atlas_threshold][l_rate]['macro_avg_support'] = macro_avg_support\n",
    "        results[atlas_threshold][l_rate]['weighted_avg_support'] = weighted_avg_support\n",
    "        \n",
    "        atlas = None\n",
    "        l_rate = None\n",
    "        \n",
    "# Plotting after the main loop\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training losses\n",
    "for lr in learning_rates:\n",
    "    plt.plot(train_losses[str(lr)], label=f'Training Loss - LR: {lr}')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss vs. Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot test accuracies\n",
    "for lr in learning_rates:\n",
    "    plt.plot([100] * len(test_accuracies[str(lr)]), test_accuracies[str(lr)], 'o-', label=f'Test Accuracy - LR: {lr}')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy vs. Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "        \n",
    "print(\"----Final Result----\")\n",
    "pprint.pprint(results)\n",
    "\n",
    "sorted_data = [(key, subkey, values['accuracy']) for key, subdata in results.items() for subkey, values in subdata.items()]\n",
    "sorted_data.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"----Sorted Accuracy----\")\n",
    "for key, subkey, accuracy in sorted_data:\n",
    "    print(f\"Key: {key}, Subkey: {subkey}, Accuracy: {accuracy}\")\n",
    "    \n",
    "# Create a list of tuples containing key, subkey, and corresponding details\n",
    "sorted_data = [(key, subkey, values) for key, subdata in results.items() for subkey, values in subdata.items()]\n",
    "\n",
    "# Sort the list based on the accuracy\n",
    "sorted_data.sort(key=lambda x: x[2]['accuracy'], reverse=True)\n",
    "\n",
    "# Create a new dictionary with the sorted order\n",
    "sorted_data_dict = {f\"{key}-{subkey}\": values for key, subkey, values in sorted_data}\n",
    "print(\"----Sorted Dict----\")\n",
    "pprint.pprint(sorted_data_dict)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
